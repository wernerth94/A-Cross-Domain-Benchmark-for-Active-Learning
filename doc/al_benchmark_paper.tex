\documentclass[]{article}
\usepackage[a4paper, total={6.5in, 8.5in}]{geometry}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{float}
\usepackage{algorithm}
\usepackage{algorithmic}

\title{Towards Comparable Active Learning}

\begin{document}

\maketitle

\section{Introduction}


\subsection{Contribution}
\begin{itemize}
	\item Benchmark suit for Active Learning
	\item Properly optimized classification models
	\item Multiple use-cases: Single Dataset, Dataset Transfer
	\item Multiple Domains: Tabular, Image, Text
\end{itemize}

\section{Related Work}

\section{Overview}

\subsection{Problem Description}
We assume a dataset $\mathcal{D} := (\mathcal{U}, \mathcal{L})$ consisting of a large pool of unlabeled data $\mathcal{U}$ and a small set of labeled data called the seed set with $|\mathcal{L}| \ll |\mathcal{U|}$. 
For the purpose of this work, only fully labeled datasets are used and the labels for $\mathcal{U}$ are suppressed until they are selected for labeling.
For evaluation purposes we also assume a held-out test set $(x_{test}, y_{test}) \in (\mathbb{R}^M, \mathbb{R}^C)$
We consider only classification problems, hence the instances of a dataset have the form $x \in \mathbb{R}^M$ for $\mathcal{U}$ and $(x, y) \in (\mathbb{R}^M, \mathbb{R}^C)$ for $\mathcal{L}$.
To perform classification, a model $\phi_\theta := \mathbb{R}^M \rightarrow \mathbb{R}^C$ is used. To fit the model, it is parameterized by $\theta$ and subjected to loss $l(y, \phi_\theta(x)) := \mathbb{R}^C \times \mathbb{R}^C \rightarrow \mathbb{R}$. For this work, categorical cross-entropy (CE) is used.
For evaluating classification performance, we use Accuracy \\ [1mm]
Active learning is defined as sequentially removing single instances $x_i \in \mathcal{U}_i$, requesting their label $y_i$ and adding them to the labeled pool $\mathcal{L}_{i+1} := \mathcal{L}_i \cup (x_i, y_i)$ until a fixed budget $B$ is exhausted $i := 0 \ldots B$.
After each added instance the classification model is retrained according to section \ref{sec:training_the_classifier} and its performance is measured on a held-out test set.
The quality of an active learning algorithm is evaluated by an "anytime" protocol that incorporates classification performance at every iteration, not just the final performance after the budget is exhausted.
We employ the normalized area under the accuracy curve (AUC):
\begin{equation}
	\operatorname{auc}(\mathcal{U}, \mathcal{L}, \phi, B) := \frac{1}{B} \sum_{i=1}^{B} acc(y_{test}, \phi_i(x_{test}))
\end{equation}
Where $\phi_i$ is the retrained classification model after the i-\textit{th} instance was selected. \\ [1mm]
We define the active learning process as an adapted reinforcement learning loop $(S, A, \tau, \Omega)$ where an environment will expose a state $s \in S$ to an agent $\Omega$, which will choose actions $a \in A$.
One iteration consists of the environment sampling a subset of size $\tau$ of unlabeled instances $x_i \underset{\tau}{\sim} \mathcal{U}_i$, constructing the state $s_i := \omega(u_i)$ and presenting it to the agent to select an action $a_i := \Omega(s_i)$.
The action $a_i$ is the index of the selected instance in $u_i$ out of all possible indices $A := [0 \ldots \tau]$.
This process is repeated $B$ times $i := [0 \ldots B]$.

\section{Methodology}

\subsection{Classification Model}
The classifier is constructed according to two kinds of information.
The general class of model (Dense, Convolutional, Attention, ...), and the configuration of the model (number of layers, size of each layer, ...). \\
The model class and exact configuration is determined by the dataset, i.e. tabular datasets will prescribe a dense model.
If special capabilities of the model are needed (i.e. Monte-Carlo Dropout), an extension of the given model class can be provided to the framework. \\ [1mm]
To ensure comparability between models, the model's configuration should not be changed or an additional evaluation of the new configuration should be conducted to compare the baseline expressivity.

\subsection{State Space}
Since every AL agent needs a different state space our environment exposes a callback-function that gives the agent full control over how the state is constructed. \\ [1mm]
The callback includes the following information:
\begin{itemize}
	\item The current sample of IDs that point to the presented unlabeled datapoints
	\item The entire labeled dataset $\mathcal{L}$
	\item The entire unlabeled dataset $\mathcal{U}$
	\item A histogram of labeled points per class (count)
	\item The available budget
	\item Number of added datapoints $|\mathcal{L}| - |\mathcal{S}|$
	\item The initial validation accuracy and current validation accuracy
	\item The current classification model including all model weights
	\item The current optimizer including it's full state
\end{itemize}
Every agent needs to implement this callback that transforms the given information into a state tensor that will be directly consumed by the agent to make it's prediction.

\subsection{Training the Classifier}\label{sec:training_the_classifier}
\subsection{Evaluation}\label{sec:evaluation}


\section{Ablation Studies}
\begin{itemize}
	\item Weird drop of performance for multiples of batch size (drop\_last in DataLoader)
	\item Reduction of the test set for speed
\end{itemize}


\bibliographystyle{plain}
\bibliography{main.bib} 


\end{document}