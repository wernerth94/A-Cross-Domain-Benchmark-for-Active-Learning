\documentclass[]{article}
%\usepackage[a4paper, total={6.5in, 8.5in}]{geometry}

%\usepackage{PRIMEarxiv}

\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage[normalem]{ulem}
\useunder{\uline}{\ul}{}
\usepackage{makecell}
\usepackage{arydshln}
\usepackage{neurips_data_2024}
\usepackage{paralist}
\usepackage{cleveref}
\usepackage{todonotes}

\graphicspath{/}

\newcommand{\R}{\mathbb{R}}
\newcommand{\EE}{\mathbb{E}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}
\newcommand{\U}{\mathcal{U}}
\newcommand{\LL}{\mathcal{L}}
\newcommand{\test}{\text{test}}
\newcommand{\train}{\text{train}}

%\pagestyle{fancy}
%\thispagestyle{empty}
%\rhead{ \textit{ }} 
%\fancyhead[LO]{Running Title for Header}

\title{A Cross-Domain Benchmark for Active Learning}

\author{%
	Anonymous Authors
	Thorben Werner 
	\thanks{Institute of Computer Science - Information Systems and Machine Learning Lab (ISMLL)} 
	\\
	University of Hildesheim\\
	Universitätsplatz 1\, 31141 Hildesheim \\
	\texttt{werner@ismll.de} \\
	\And
	Johannes Burchert$^*$ \\
	University of Hildesheim\\
	Universitätsplatz 1, 31141 Hildesheim \\
	\texttt{burchert@ismll.de} \\
	\And
	Maximilian Stubbemann$^*$ \\
	University of Hildesheim\\
	Universitätsplatz 1, 31141 Hildesheim \\
	\texttt{stubbemann@ismll.de} \\
	\AND
	Lars Schmidt-Thieme$^*$ \\
	University of Hildesheim\\
	Universitätsplatz 1, 31141 Hildesheim \\
	\texttt{schmidt-thieme@ismll.uni-hildesheim.de}
}

\begin{document}
	

% TODOs nach Priorität
% Advantage-Matrix von Ji et. al. neu aufarbeiten. Wo sind die Unterschiede? Ist das CD-Diagramm besser/schlechter?
% Eventuell Batchsize 1 von den anderen Ergebnissen separieren. Der Vergleich macht mehr Sinn, als einfach die Batchsizes zu mitteln
% Tabelle 1 für Algorithmen statt Domänen. Das zeigt, dass immer entweder BADGE oder Margin (unsere Top 2) fehlt
% Unsere Rankings mit Literatur vergleichen. Wie anders sind wir? -> Evtl. besten Alg. pro Benchmark sammeln und vergleichen
% Fig. 1: Schlechte Performance am Anfang verwirrt nur. Entweder BADGE benutzen, anders plotten, oder im Text in Kontext setzen
% Oracle Forecast im Appendix erklären 

\maketitle


\begin{abstract}

Active Learning (AL) deals with identifying the most informative samples for
labeling to reduce data annotation costs for supervised learning tasks. AL
research suffers from the fact that lifts from literature generalize poorly and
that only a small number of repetitions of experiments are conducted. To overcome
these obstacles, we propose \emph{CDALBench}, the first active learning benchmark
which includes tasks in computer vision, natural language processing and tabular
learning. Furthermore, by providing an efficient, greedy oracle, \emph{CDALBench}
can be evaluated with 50 runs for each experiment. We show, that both the
cross-domain character and a large amount of repetitions are crucial for
sophisticated evaluation of AL research. Concretely, we show that the
superiority of specific methods varies over the different domains, making it
important to evaluate Active Learning with a cross-domain benchmark.
Additionally, we show that having a large amount of runs is crucial. With only
conducting three runs as often done in the literature, the superiority of
specific methods can strongly vary with the specific runs. This effect is so strong, that, depending on the seed, even a well-established method's performance can be significantly better and significantly
worse than random for the same dataset. 
\end{abstract}

% \begin{abstract}

% 	Active Learning has received significant attention in the field of machine learning. Its goal is to select the most informative samples for labeling, thereby reducing data annotation costs. 
% 	However, it has been brought to attention multiple times that reported lifts from literature generalize poorly and display high variance, leading to an inconclusive landscape in Active Learning research.
% 	Based on recent insights for reliable evaluation for Active Learning, this work extends experimentation from the commonly used image domain to a wide spectrum of domains.
% 	%This work follows up on recent benchmarking suites for Active Learning, applying the proposed solutions and evaluation protocols to a wide spectrum of domains.
% 	Additionally, we provide an analysis of how many repetitions an Active Learning experiment needs in order to derive conclusive results and propose that previous benchmarks have not met the necessary number of repetitions.
% 	%evidence that existing benchmarks might not have repeated their experiments often enough.
% 	To the best of our knowledge, we propose the first AL benchmark that applies state-of-the-art evaluation on algorithms in 3 major domains: Tabular, Image, and Text as well as synthetic data.
% 	We report empirical results for 11 widely used algorithms on 7 real-world and 2 synthetic datasets and aggregate them into domain-specific and overall rankings of AL algorithms.
% 	\end{abstract}

% \begin{abstract}
% Active Learning has received significant attention in the field of machine
% learning. Its goal is to select the most informative samples for labeling,
% thereby reducing data annotation costs. However, it has been brought to
% attention multiple times that reported lifts from literature generalize poorly
% and display high variance, leading to an inconclusive landscape in Active
% Learning research. Based on recent insights for reliable evaluation for Active
% Learning, this work extends experimentation from the commonly used image domain
% to a wide spectrum of domains.
% %This work follows up on recent benchmarking suites for Active Learning, applying the proposed solutions and evaluation protocols to a wide spectrum of domains.
% Additionally, we provide an analysis of how many repetitions an Active Learning
% experiment needs in order to derive conclusive results and propose that previous
% benchmarks have not met the necessary number of repetitions.
% %evidence that existing benchmarks might not have repeated their experiments often enough.
% To the best of our knowledge, we propose the first AL benchmark that applies
% state-of-the-art evaluation on algorithms in 3 major domains: Tabular, Image,
% and Text as well as synthetic data. We report empirical results for 10 widely
% used algorithms on 7 real-world and 2 synthetic datasets and aggregate them into
% domain-specific and overall rankings of AL algorithms.
% \end{abstract}


\section{Introduction}\label{sec:introduction}
Deep neural networks (NN) have produced state-of-the-art results on many
important supervised learning tasks. Since Deep NNs usually require large
amounts of labeled training data, Active Learning (AL) deals with
selecting the most informative samples out of a large pool of unlabeled data, so
that only these samples need to be labeled. It has been shown that a small
labeled set of this nature can be used to train well-performing models.
%\cite{beck2021effective, hu2021towards, li2022empirical, zhou2021towards}.
% On top of providing a principled way to label unlabeled datasets, active
% learning is one of the two major approaches besides semi-supervised learning to
% make deep learning models more data efficient by requiring only a limited set of
% manually labeled data.
%Both approaches are at their core orthogonal and can freely be combined and therefore we should continue our research efforts for both approaches. \\
In the last decade, many different algorithms for AL have been proposed and
almost every method has reported lifts over all its predecessors. \footnote{Out
of all considered algorithms for this paper, only BALD \cite{gal2017deep} did
not claim a new state-of-the-art (SOTA) performance in their result section.} However, real
insights into the current state of AL are hard to draw from these works, due to
the following reasons:
\begin{inparaenum}
\item\label{prob1al} These works do not use a standardized evaluation setting with fixed
datasets and baseline approaches.
\item Due to computational constraints, a lot of works perform only a small amount of experimental
runs, hence it is questionable wether the superiority of a specific approach can
be concluded from the conducted experiments.
\item The works are only evaluated in a specific domain, such as computer vision
or language processing. However, AL is a general principle of supervised
learning, and thus methods should be evaluated in multiple domains to assess
their capabilities.
\end{inparaenum}
% %{\let\thefootnote\relax\footnotetext{Code available at: anonymous}}
% %\let\thefootnote\relax\footnotetext{Code available at: \url{https://github.com/wernerth94/comparable-active-learning}}
% % (ii) Most of the research is focused on a specific application domain like
% % computer vision, even though AL is a general principle for supervised learning
% % (iii) the reported results can be subject to very high variances across restarts,
% % still A lot of papers do only a hand full of runs, which makes questions the
% % validity of the drawn conclusions.
% % (iv) are not always compared against important baselines like margin
% sampling \cite{wang2014new}.

% These outlined issues lead to an
% inconclusive landscape of AL algorithms, where the vast majority of reported
% lifts are neither statistically significant, nor prove to be generalizable. This
% makes it very hard to identify the best AL algorithm, or even identifying
% state-of-the-art algorithms. 
While multiple benchmark suites have been proposed to solve
problem~\ref{prob1al}, to the best of
our knowledge, all of them are either limited in the domains they consider or do
not contain enough runs to generate conclusive results.
Hence, the current SOTA in AL is still not well-understood and principled
shortcomings of different algorithms and wether they are domain-independent are currently not identified.

Here we step in with \emph{CDALBench}, an AL benchmark which covers
multiple application domains and reports a large amount of runs per
experiment, so that the significance of performance differences can be
estimated. Specifically, \emph{CDALBench} consists of datasets from
computer vision, natural language processing and the tabular domain. We provide
our datasets both in normal format %(i.e. consisting of text, images etc)
as well as ``embedded'' by a fixed embedding model, enabling evaluation of AL methods in
this semi-supervised setting.
Furthermore, we propose two novel synthetic datasets to highlight general
challenges for AL methods.
\begin{table}
	\centering
	\caption{Comparison of our benchmark with the existing literature. Oracle curves
		serve as an approximation of the best possible AL algorithm. Our benchmark contains 9 
		datasets (14 including the encoded versions). ``Semi"
		indicates whether the paper is employing any form of self- or semi-supervised
		learning. A ``-" for repetitions means that we could not determine how often each
		experiment was repeated in the respective framework. \emph{CDALBench} is the only
		benchmark which evaluates a high number of runs and considers
		all 5 domains.}
	\label{tab:benchmark_comparison}
	\resizebox{\columnwidth}{!}{
		\begin{tabular}{lccc|cllll|cl}
			Paper & Sampling & \#Data & \#Alg & \multicolumn{1}{l}{Img} & Txt & Tab & Synth & Semi & Oracle & Repetitions \\ \hline
			\multicolumn{1}{l|}{Beck et al. \cite{beck2021effective}} & batch & 4 & 7 & \checkmark & - & - & - & - & - & - \\
			\multicolumn{1}{l|}{Hu et al. \cite{hu2021towards}} & batch & 5 & 13 & \checkmark & \checkmark & - & - & - & - & 3 \\
			\multicolumn{1}{l|}{Zhou et al. \cite{zhou2021towards}} & batch & 3 & 2 & \checkmark & \checkmark & - & - & - & \checkmark & 5 \\
			\multicolumn{1}{l|}{Zhan et al. \cite{zhan2021comparative}} & sngl+batch & 35 & 18 & - & - & \checkmark & \checkmark & - & \checkmark & 10-100 \\
			\multicolumn{1}{l|}{Munjal et al. \cite{munjal2022towards}} & batch & 2 & 8 & \checkmark & - & - & - & - & - & 3 \\
			\multicolumn{1}{l|}{Li et al. \cite{li2022empirical}} & batch & 5 & 13 & \checkmark & - & - & - & \checkmark & - & - \\
			
			\multicolumn{1}{l|}{Rauch et al. \cite{rauch2023activeglae}} & batch & 11 & 5 & - & \checkmark & - & - & - & - & 5 \\
			\hdashline
			\multicolumn{1}{l|}{Ji et al. \cite{ji2023randomness}} & batch & 3 & 8 & \checkmark & - & - & - & - & - & - \\
			\multicolumn{1}{l|}{Lueth et al. \cite{luth2024navigating}} & batch & 4 & 5 & \checkmark & - & - & - & \checkmark & - & 3 \\
			\multicolumn{1}{l|}{\textbf{Ours}} & sngl+batch & 9(14) & 11 & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & 50 
		\end{tabular}
	}
\end{table}
The applied evaluation protocol in \emph{CDALBench} uses 50 runs for
each experiment. By having such a large amount of runs, we can evaluate the
significance of performance gaps and identify the best performing
approaches for each dataset as well as whole domains. 
Furthermore, we show that the small amount of runs other works do, in fact, 
produce misleading results. 
To be more specific, we show that if only 3 restarts are employed for each experiment, 
the performance of specific methods strongly varies. As we will see, even the ranking of
the different methods averaged over many datasets fluctuates with the specific set of runs. 
This effect is so strong, that, depending on the seed, even a well-established method's performance can be significantly better and significantly worse than random for the same dataset. 

To enable the computation of an oracle performance for a protocol with large amounts of restarts,
we propose a \emph{greedy oracle algorithm} 
which uses only a small amount of search steps to estimate the
optimal solution. While being more time-efficient than established oracle
functions, it possibly underestimates the real upper bound performance. 
However, as our experiments will show, it is still outperforming all current AL methods
by at least 5\% and thus is suitable as an upper bound.

Our experimental evaluation shows that there exists no clear SOTA
method for AL. The superiority of methods is strongly dataset- and
domain-dependent with the outstanding observation, that the image domain works
fundamentally different than the tabular and text domain. Here, the best
performing approach for text and tabular data, namely \emph{margin sampling}, is
significantly outperformed by \emph{least confident sampling}, which does not belong to
the top performing approaches in any other domain. Thus, using the performance
of AL approaches on the image domain as a proxy of AL
in general, as it is often done \cite{beck2021effective, munjal2022towards, li2022empirical, ji2023randomness, luth2024navigating}, is questionable. To further analyze
performance of common methods, we propose \emph{Honeypot} and \emph{Diverging Sine}, two
synthetic datasets, designed to be challenging for naive decision-boundary- and clustering-based approaches respectively. Hence, they provide insights in principled shortcomings of AL methods.

In summary, \emph{CDALBench} is an experimental framework which includes an
efficient oracle approximation, multiple application domains, enough repetitions to
draw valid conclusions and two synthetic tasks to highlight shortcomings of AL methods. 
By being the first benchmark to providing these points in one code-base, 
we believe that \emph{CDALBench} is a major step forward of assessing the overall state of AL research, independent of
specific application domains. \emph{CDALBench} is publicly available under %\url{https://github.com/wernerth94/A-Cross-Domain-Benchmark-for-Active-Learning/}. 
\url{https://anonymous.4open.science/r/A-Cross-Domain-Benchmark-for-Active-Learning-CCB1/README.md}
\\ [1mm]
Our contributions include the following:
%
\begin{enumerate}
\item\label{cont:repetitions} We show that the small number of repetitions that previous works have employed is not
sufficient for meaningful conclusions. Sometimes even making it impossible to assess if a performance is above or below random.
\item\label{cont:oracle} We propose an efficient and performant oracle which is constructed iteratively in a greedy fashion,
overcoming major computational hurdles.
% one of the major obstacles for sufficient repetitions of AL experiments. 
% [Thorben] Das können wir so nicht sagen, glaube ich. Niemand verlangt von uns, dass wir das Oracle auch 50 mal wiederholen
\item\label{cont:domains} We propose \emph{CDALBench}, the first general benchmark providing tasks
in the domains of image, text and tabular learning. It further contains
synthetic and pre-encoded data to allow for a sophisticated evaluation of AL
methods. Our experiments show, that there is no clear SOTA method
for AL across different domains.
\item\label{cont:synthdata} We propose \emph{Honeypot} and \emph{Diverging Sin}, two synthetic datasets
designed to hinder AL by naive decision-boundary- or
clustering-based approaches respectively. Thus, they provide an important tool to identify 
shortcomings of existing AL methods.
\end{enumerate}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Problem Description}
Given two spaces $\X, \Y$, $n=l+u$ data points with $l \in \mathbb{N}$ labeled examples $\mathcal{L} = \{(x_1, y_1),\ldots, (x_l,y_l)\}$, $u \in \mathbb{N}$ unlabeled examples $\mathcal{U} = \{x_{l+1},\ldots,x_{n}\}$, a model $\hat y: \X \to \Y$, a budget $\mathbb{N} \ni b \le u$ and an annotator $A: \mathcal{X} \to \mathcal{Y}$ that can label $x$. % (we consider only hard labels in the one-hot format). 
We call $x \in \mathcal{X}$, $y \in \mathcal{Y}$ predictors and labels respectively where $(x,y)$ are drawn from an unknown distribution $\rho$. 
Find an AL method $\Omega: \U^{(i)},\LL^{(i)} \mapsto x^{(i)} \in \U^{(i)}$ that iteratively selects the next unlabeled point $x^{(i)}$ for labeling
\begin{align*}
	\LL^{(i+1)} &\gets \LL^{(i)} \cup \{\left(x^{(i)}, A(x^{(i)})\right)\} \\
	\U^{(i+1)} &\gets \U^{(i)} \setminus \left\{x^{(i)}\right\} %, i \in 1{:}B
\end{align*}
with $\U^{(0)} = \operatorname{seed}(\U, s)$ and $\LL^{(0)} = \left(\U^{(0)}_i, A(\U^{(0)}_i)\right) \hspace{1mm} i \in [1, \ldots, s]$, where $\operatorname{seed}(\U, s)$ selects $s$ points per class for the initial labeled set $\LL^{(0)}$. \\
So that the average expected loss $\ell: \mathcal{Y} \times \mathcal{Y} \to \mathbb{R}$ of a machine learning algorithm fitting $\hat y^{(i)}$ on the respective labeled set $\LL^{(i)}$ is minimal: 
$$\min \quad \frac{1}{B} \sum\limits_{i=0}^B \mathbb{E}_{(x,y) \sim \rho} \ell(y, \hat y^{(i)})$$

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Work}
While multiple benchmark suites have been proposed for AL, none of them provide experiments for more than two domains.
The authors of \cite{beck2021effective}, \cite{munjal2022towards}, \cite{li2022empirical}, \cite{ji2023randomness} and \cite{luth2024navigating} even focus exclusively on the image domain.
Especially the tabular domain is underrepresented in preceding benchmarks, as only \cite{zhan2022comparative} provides experiments for it.
The interplay between AL and semi-supervised learning is similarly under-researched, as only two works exist \cite{li2022empirical, luth2024navigating}, both of them only using images.
An oracle algorithm has been proposed by two works \cite{zhou2021towards, zhan2022comparative}. 
Both of these algorithms rely on search and are computationally very expensive, while our proposed method efficiently can be constructed sequentially.
%While \cite{beck2021effective} discuss a new metric to measure AL performance, which they call ``Label Efficiency'' and provide experiments on many common configurations of data preparation, model training, and other hyperparameters, \cite{li2022empirical} focuses on combined approaches of AL and semi-supervised learning.
%The authors of \cite{hu2021towards} study models that are trained on actively learned datasets in the image and text domain.
%They test for several different properties of the models including robustness, response to compression techniques and final performance.
%\cite{zhou2021towards} proposed an oracle algorithm for AL that uses Simulated Annealing search to approximate a solution for the optimal subset of labeled data.
%Additionally, they study the generalization behavior of subsets of labeled data in the text an image domain.
The two closest related works to this benchmark are \cite{ji2023randomness} and \cite{luth2024navigating}, who also place a much higher emphasis on the problem of evaluating AL methods under high variance than their predecessors (indicated in Tab. \ref{tab:benchmark_comparison} by a dashed line).
The authors of \cite{ji2023randomness} posed a total of 11 ``recommendations" for reliable evaluation of AL methods.
We largely adapt the proposed recommendations and extend their work to multiple domains and query sizes.
For a complete list of the recommendations and our implementation of them, please refer to App. \ref{app:recommendations}.
This work also pays attention to the so-called ``pitfalls" of AL evaluation proposed in \cite{luth2024navigating}.
For a complete list of the pitfalls and our considerations regarding them, please refer to App. \ref{app:pitfalls}.
To the best of our knowledge, we are the first to extend reliable SOTA (based on \cite{ji2023randomness, luth2024navigating}) experimentation to a total of 5 data domains and a high number of repetitions per experiment.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Few Repetitions are not Sufficient for Meaningful Results}\label{sec:restarts}
To evaluate how many repetitions are necessary to obtain conclusive results in an AL experiment, we computed 100 runs of our top-performing AL method on one dataset.
Our best method is margin sampling and we chose the Splice dataset for its average size and complexity. \\
This allows us firstly, to obtain a very strong estimation of the ``true" average performance of margin sampling on this dataset and secondly, to draw subsets from this pool of 100 runs.
Setting the size of our draws to $\alpha$ and sampling uniformly, we can approximate a cross-validation process with $\alpha$ repetitions.
Each of these draws can be interpreted as a \textbf{reported result in AL literature} where the authors employed $\alpha$ repetitions.
Figure \ref{fig:restarts} shows the ``true" mean performance of margin sampling (green) in relation to random sampling (black) and the oracle performance (red).
We display 5 random draws of size $\alpha$ in blue.
We can observe that even for a relatively high number of repetitions the variance between the samples is extremely high, resulting in some performance curves being worse than random and some being significantly better.
When setting $\alpha = 50$ we observe all samples to converge close to the true mean performance. 
\begin{figure}
	\centering
	\caption{Random draws from a pool of 100 runs for margin sampling on the Splice dataset with different numbers of repetitions ($\alpha=\{3,5,50\}$). Green curves are the mean performance of all 100 runs, while the samples are blue. Even with 3 or 5 repetitions, we can observe that single draws for margin sampling display below-random performance (black), while the true mean should be above random.}
	\label{fig:restarts}
	\includegraphics[width=\linewidth]{img/ablation_restarts}
\end{figure}
In addition to this motivating example, we carried out our main evaluation (Tab. \ref{tab:results}) multiple times by sampling 3 from our available runs uniformly at random and comparing the results.
We found significant differences in the performance of AL methods on individual datasets, as well as permutations in the final ranking.
This partly explains the ongoing difficulties in reproducing results for AL experiments and benchmarks.
The details can be found in App. \ref{app:rank_difference}.
For this benchmark we employ 50 repetitions of every experiment.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Seeding vs. Repetitions}\label{sec:reproducibility}
Considering the high computational cost of 50 repetitions, another approach to ensure consistency between experiments would be to reduce the amount of variance in the experiment by keeping as many subsystems (weight initialization, data splits, etc.) as possible fixed with specialized seeding. \\
We describe a novel seeding strategy in Appendix \ref{app:seeding_strategy} that is capable of tightly controlling the amount variance in the experiment.
However, previous works have noted that an actively sampled, labeled set does not generalize well between model architectures or even different initializations of the same model (\cite{zhou2021towards, lowell2018practical}), providing a bad approximation of the quality of an AL method (i.e. measured performances for an AL method might not even transfer to a different model initialization).
Hence, we opt for letting the subsystems vary in controlled way (For details, please refer to App. \ref{app:seeding_strategy}) and combine that with a high number of repetitions to obtain a good average of the generalization performance of each AL method. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{CDALBench: A Cross-Domain Active Learning Benchmark}\label{sec:datasets}
\begin{wraptable}{R}{0.65\textwidth}
	\vspace{-0.7cm}
	\caption{Employed model, chosen budget and available query sizes for each dataset}
	\vspace{0.1cm}
	\label{tab:batch_sizes}
	{\scriptsize
		\begin{tabular}{l|l|c|c|c|c|c|c|c|c}
			& Model  & B & 1 & 5 & 20 & 50 & 100 & 500 & 1K \\
			\hline
			Semi DNA    & Linear & 40 & o & o &&&&& \\
			Semi Splice & Linear & 100 & o & o & o &&&&\\
			TopV2       & BiLSTM & 200 & o & o & o & o &&& \\
			Splice      & MLP    & 400 & o & o & o & o & o && \\
			DNA         & MLP    & 300 & o & o & o & o & o && \\
			USPS        & MLP    & 400 & o & o & o & o & o && \\
			Semi Cifar10& Linear & 450 & o & o & o & o & o && \\
			Semi FMnist & Linear & 500 & o & o & o & o & o && \\
			Semi USPS   & Linear & 600 & o & o & o & o & o && \\
			News        & BiLSTM & 3K &&& o & o & o & o &\\
			FMnist      & ResNet18& 10K &&&&&& o & o\\
			Cifar10     & ResNet18& 10K &&&&&& o & o \\
		\end{tabular}
	}
	\vspace{-0.55cm}
\end{wraptable}
A detailed description of the preprocessing of each dataset can be found in Appendix \ref{app:hyperparameters}. \\ [1mm]
\textbf{Tabular:}
AL research conducted on tabular data is sparse (only \cite{ashdeep} from the considered baseline papers). 
We, therefore, introduce a set of tabular datasets that we selected according to the following criteria:
(i) They should be solvable by medium-sized models in under 1000 samples, (ii) the gap between most AL methods and random sampling should be significant (potential for AL is present) and (iii) the gap between the AL methods and our oracle should also be significant (research on these datasets can produce further lifts).
We use \textbf{Splice}, \textbf{DNA} and \textbf{USPS} from LibSVMTools \cite{libsvmtools}.\\
\textbf{Image:}
We use \textbf{FashionMNIST} \cite{xiao2017fashion} and \textbf{Cifar10} \cite{krizhevsky2009learning}, since both are widely used in AL literature.\\
\textbf{Text:}
We use \textbf{News Category} \cite{misra2022news} and \textbf{TopV2} \cite{chen-etal-2020-low-resource}.
Text datasets have seen less attention in AL research, but most of the papers that evaluate on text (\cite{hu2021towards}, \cite{zhou2021towards}) use at least one of these datasets. 
We use both, as they complement each other in size and complexity. \\ [1mm]
%
We would like to point out that these datasets are selected for speed of computation (both in terms of the required classifier and the necessary budget to solve the dataset). 
We are solely focused on comparing different AL methods in this paper and do not aim to develop novel classification models on these datasets.
Our assumption is that a well-performing method in our benchmark will also generalize well to larger datasets and classifiers, because we included multiple different data domains, classifier types and sizes in our experiments. \\ [1mm]
Adapting the semi-supervised setting from \cite{hacohen2022active}, we offer all our datasets un-encoded (normal) as well as pre-encoded (semi-supervised) by a fixed embedding model that was trained by unsupervised contrastive learning .
The text datasets are an exception to this, as they are only offered in their encoded form.
Pre-encoded datasets enable us to test small query sizes on more complex datasets like Cifar10 and FashionMnist.
They also serve the purpose of investigating the interplay between semi-supervised learning techniques and AL, as well as alleviating the cold-start problem described in \cite{luth2024navigating} as they require a way smaller seed set.
The classification model for every encoded dataset is a single linear layer with softmax activation.
The embedding model was trained with the SimCLR \cite{chen2020simple} algorithm adopting the protocol from \cite{hacohen2022active}. 
%For Cifar10 and FashionMnist we adapt the reported hyperparameters from \cite{hacohen2022active} and for the tabular datasets we use random search to optimize the hyperparameters.
To ensure that enough information from the data is encoded by our embedding model, the quality of embeddings during pretext training was measured after each epoch.
To this end, we attached a linear classification head to the encoder, fine-tuned it to the data and evaluated this classifier for test accuracy. 
The checkpoint of each encoder model will be provided together with the framework. \\ [1mm]
Every dataset has a fixed size for the seed set $\LL^{(0)}$ of 1 sample per class, with the only exceptions being un-encoded FashionMnist and Cifar10 with 100 examples per class to alleviate the cold-start problem in these complex domains. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Query Sizes}\label{sec:batch_sizes}
We selected query sizes for each dataset to accommodate the widest range possible that results in a reasonable runtime for low query sizes and allows for at least 4 round of data acquisition for high query sizes.
The available query sizes per dataset can be found in Table \ref{tab:batch_sizes}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Realism vs. Variance}\label{sec:realism}
We would like to point out that some design choices for this framework prohibit direct transfer of our results to practical applications. 
This is a conscious choice, as we think that this is a necessary trade-off between realism and experiment variance.
We would like to highlight the following design decisions: \\
(i) Creating test and validation splits from the full dataset rather than only the labeled seed set (following \cite{luth2024navigating}). Fully fledged test and validation splits are unobtainable in practice, but they provide not only a better approximation of the methods generalization performance, but also a better foundation for hyperparameter tuning, which is bound to reduce variance in the experiment. \\
(ii) Choosing smaller classifiers instead of SOTA models. Since we are not interested in archiving a new SOTA in any classification problem, we instead opt to use smaller classifiers for the following reasons:
Smaller classifiers generally exhibit more stable training behavior, on average require fewer sampled datapoints to reach their full-dataset-performance and have faster training times.
For every dataset, the chosen architecture's hyperparameters are optimized to archive maximum full-dataset performance.
Generally, we use MLPs for tabular, RestNet18 for image and BiLSTMs for text datasets.
Every encoded dataset is classified by a single linear layer with softmax activation.
The used model for each dataset can be found in Tab. \ref{tab:batch_sizes}.
For a detailed description and employed hyperparameters please refer to Appendix \ref{app:hyperparameters}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Evaluation Protocol}\label{sec:evaluation}
Following \cite{zhou2021towards}, the quality of an AL method is evaluated by an ``anytime protocol" that incorporates classification performance at every iteration, as opposed to evaluating final performance after the budget is exhausted.
We employ the normalized area under the accuracy curve (AUC):
\begin{equation}\label{eq:auc}
	\operatorname{AUC}(\D_\test, \hat y, B) := \frac{1}{B} \sum_{i=1}^{B} \operatorname{Acc}(\D_\test, \hat y^{(i)})
\end{equation}
%The AUC incorporates performance in early stages (low budget) as well as capabilities to push the classifier in later stages (high budget).
%AL algorithms have to perform well in both scenarios. \\ [1mm]
Since AUC is still influenced by the budget, we define a set of rules to set this hyperparameter upfront, so that we are not favoring a subset of methods by handcrafting a budget.
In this work, we choose the budget per dataset to be the first point at which one of 2 stopping conditions apply: (i) an method (except oracle) manages to reach 99\% of the full-dataset-performance (using the smallest query size) or (ii) the best method (except oracle) did not improve the classifier's accuracy by at least 2\% in the last 20\% of iterations.
The first rule follows \cite{ji2023randomness}, while the second rule prevents excessive budgets for cases with diminishing returns in the budget.
The resulting budgets can be found in Tab. \ref{tab:batch_sizes}. \\ [1mm]
As described in Sec. \ref{sec:restarts}, we repeat each experiment 50 times.
Each repetition retains the train/test split (often given by the dataset itself), but creates a new validation split that is sampled from the entire dataset (not just the seed set $\LL^{(0)}$). \\ [1mm]
%
Apart from plotting standard performance curves and reporting their AUC values per dataset in App. \ref{app:all_results}, we primarily rely on ranks to aggregate the performance of an AL method across datasets.
For each dataset and query size, the AUC values of all AL methods are sorted and assigned a rank based on position, with the best rank being 1.
These ranks can safely be averages across datasets as they are no longer subjected to scaling differences of each dataset.
Additionally, we employ Critical Difference (CD) diagrams (like Fig. \ref{fig:ranks_by_domain}) for statistical testing.
CD diagrams \cite{IsmailFawaz2018deep} use the Wilcoxon signed-rank test, which is a variant of the paired T-test, to find significant differences of ranks between AL methods.
For a detailed description of how every CD diagram is created, please refer to App. \ref{app:cd_diagrams}. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{A Greedy Oracle Algorithm}\label{sec:oracle} 
Using additional resources, like excessive training time, or direct access to a labeled test set, an oracle method for AL finds the oracle set $\mathcal{O}_b$ for a given dataset, model, and
training procedure that induces the highest AUC score for a given budget.
However, due to the combinatorial nature of the problem, this is computationally
infeasible for realistic datasets. Hence, previous works have proposed
approximations to this oracle sequence. \cite{zhou2021towards} used simulated
annealing to search for a subset with maximal test accuracy and used the best solution
after a fixed time budget. 
Even though their reported performance curves display
a significant lift over all other AL methods, we found the
computational cost of reproducing this oracle for all our datasets to be
prohibitive (The authors reported the search to take several days per dataset on
8 V100 GPUs). In this paper, we propose a greedy oracle algorithm that
constructs an approximation of the optimal set in an iterative fashion. 
Our oracle algorithm
uniformly samples at iteration $i$ a subset $\mathcal{U}_S$ of size $\tau$ of the not already
labeled data points $\mathcal{U}^{(i)}$.
Then it recovers the label $y$ for each of the sampled $u \in \mathcal{U}_S$ and selects the
point $u$ for which the classifier $\hat{y}^{(i)}$ trained on
$\mathcal{L}^{(i)} \cup \{u\}$ has maximal performance.
%Our oracle algorithm evaluates every data point $u_k =
%\operatorname{unif(\mathcal{U}) \quad k \in [1 \ldots \tau]}$ in a subsample of
%unlabeled points by recovering the label, fitting the classifier $\hat y$ on
%$\mathcal{L}^{(i)} \cup \{u_k\}$ and directly measuring the resulting test
%performance.
%The data point that incurs the largest lift in test performance is
%selected and added to the labeled pool for that iteration.
Due to the algorithms greedy nature (considering only the next point to pick),
our oracle frequently encounters situations where every point in $u$ would
incur a negative lift (worsening the test performance). This can happen, for
example, if the oracle picked a labeled set that enables the classifier to
correctly classify a big portion of easy samples in the test set, but now fails
to find the next \textbf{single} unlabeled point that would enable the
classifier to succeed on one of the hard samples. This leads to a situation,
where no point can immediately incur an increase in test performance and
therefore the selected data point can be considered random. To circumvent this
problem, we use our best-performing AL method (margin sampling
\cite{wang2014new}) as a fallback option for the oracle. Whenever the oracle
does not find an unlabeled point that results in an increase in performance, it
defaults to margin sampling from the entire unlabeled pool $\mathcal{U}^{(i)}$ in that iteration.
The resulting greedy algorithm
constructs an approximation of the optimal labeled set that consistently
outperforms all other algorithms by a significant margin, while requiring
relatively low computational cost ($\mathcal{O}(B\tau)$).
We fix $\tau = 20$ in this work, as this gives us an average lift of 5\% over the
best performing AL method per dataset (which is significant for AL
settings) and we expect diminishing returns for larger $\tau$. The pseudocode
for our oracle can be found in App. \ref{app:pseudocode}.
%In the algorithm $\operatorname{Train}(\LL, \hat y_\theta)$ trains the classification model $\hat y_\theta$ on $\LL$. \\
%Alg. \ref{alg:oracle} replaces the acquisition function $\Omega$ in the AL loop (Appendix \ref{app:pseudocode} Alg. \ref{alg:active_learning}).
Even though our proposed algorithm is more efficient than other approaches, the
computational costs for high budget datasets like Cifar10 and FashionMnist meant
that we could not compute the oracle for all 10000 datapoints. To still provide
an oracle for these two datasets, we select two points per iteration instead of
one and stop the oracle computation at a budget of 2000. The rest of the curve
is forecast with a 2-stage linear regression that asymptotically approaches the
upper bound performance of the dataset. A detailed description can be found in
App. \ref{app:oracle_forecasting}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experiments}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Implementation Details}\label{sec:implementation_details}
At each iteration $i$ the AL method picks an unlabeled datapoint based on a fixed set of information $\{\mathcal{L}^{(i)}, \mathcal{U}^{(i)}, B, |\mathcal{L}^{(i)}|-|\mathcal{L}^{(1)}|, \text{acc}^{(i)}, \text{acc}^{(1)}, \hat y^{(i)}, \text{opt}_{\hat y}\}$, where $\text{opt}_{\hat y}$ is the optimizer used to fit $\hat y^{(i)}$.
This set grants full access to the labeled and unlabeled set, as well as all parameters of the classifier and the optimizer.
Additionally, we provide meta-information, like the size of the seed set through $|\mathcal{L}^{(i)}|-|\mathcal{L}^{(1)}|$, the remaining budget though the addition of $B$ and the classifiers potential through $\text{acc}^{(1)}$ and $\text{acc}^{(i)}$.
We allow AL methods to derive information from this set, e.g. predictions of the classifier $\hat y^{(i)}(x); \hspace{2mm} x \in \mathcal{U}^{(i)} \cup \mathcal{L}^{(i)}$, clustering, or even training additional models.
However, the method may not incorporate external information e.g. other datasets, queries to recover additional labels, additional training steps for $\hat y$, or the test/validation set. \\
For our study we selected AL methods with good performances reported by multiple different sources that can work with the set of information stated above.
For a list of all AL methods, please refer to Table \ref{tab:results}, with detailed descriptions being found in Appendix \ref{app:acquisition_functions}. \\ [1mm]
%
The model $\hat y^{(i)}$ can be trained in two ways. Either the parameters of the model are reset to a fixed initial setting $\hat y^{(0)}$ after each AL iteration and the classifier is trained from scratch with the updated labeled set $\mathcal{L}^{(i)}$, or the previous state $\hat y^{(i-1)}$ is retained and the classifier is fine-tuned on $\mathcal{L}^{(i)}$ for a reduced number of epochs.
In this work, we use the fine-tuning method for un-encoded datasets to save computational time, while we use the from-scratch training for encoded datasets since they have very small classifiers and this approach generally produces better results.
Our fine-tuning scheme always trains for at least one epoch and employs an aggressive early stopping with a patience of 2 afterwards.
%This work employs a validation split that is produced from the source dataset itself before active learning instead of splitting it from the seed set or omitting it altogether.
%Even though the use of a fully labeled validation set might be regarded as impractical, since such a set will never exist during deployment, we strongly advocate for using it in \textbf{research}, as it can be used for hyperparameter tuning, resulting in more stable hyperparameters and reducing the overall training stochasticity.
%We optimize all hyperparameters on our fully labeled validation sets using grid search.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Results on Real-world Data}
In Table \ref{tab:results} we provide the rank of each AL method per dataset. 
Please note, that we are averaging not only over runs, but also over query sizes per dataset, impacting AL methods that do not adapt well to a wide range of query sizes.
For the results per query size, please refer to App. \ref{app:auc_by_query_size}. 
As mentioned in contribution \ref{cont:domains}, our results on real-world data show significant differences in the performance of methods between data domains:
Not only do some methods overperform on some domains (like least confidence (LC) sampling on images), but the Top-3 of methods (except oracle) does not contain the same three methods for \textbf{any} two domains.
Most interestingly, the image domain, which received most of the attention in benchmarking so far could even be considered an outlier, as this is the only domain where the Top-1 method changes.
This highlights the dire need for diverse data domains in AL benchmarking. \\
Results for the semi-supervised domain appear mostly in line with the other 3 domains, but a closer analysis of performances split into encoded images and encoded tabular reveals the need for further research.
For details, please refer to App. \ref{app:semi_sup_analysis}.
\begin{table}[]
	\caption{Performances for AL methods on real-world datasets, aggregated for un-encoded (normal) and encoded (semi-supervised) datasets. Performance is shown as average ranks over repetitions (1.0 is the best rank). Methods are sorted by aggregated performance on un-encoded (normal) datasets. }
	\label{tab:results}
	\centering
        \resizebox{\columnwidth}{!}{
\begin{tabular}{l|lllllll|ll}
          & Splice & DNA  & USPS 		 	& Cifar10 & FMnist & TopV2 & News  & Normal & Semi \\
          \hline
Oracle   & 1.0 $\pm$ {\small 0.01} & 1.0 $\pm$ {\small 0.01} & 1.0 $\pm$ {\small 0.0}  & 1.0 $\pm$ {\small 0.0}  & 1.0 $\pm$ {\small 0.0}  & 1.0 $\pm$ {\small 0.01} & 1.0 $\pm$ {\small 0.0} & 1.0 & 2.0 \\
Margin   & 6.6 $\pm$ {\small 0.02} & 4.3 $\pm$ {\small 0.01} & 2.1 $\pm$ {\small 0.01} & 6.3 $\pm$ {\small 0.01} & 4.4 $\pm$ {\small 0.0}  & 2.4 $\pm$ {\small 0.01} & 3.7 $\pm$ {\small 0.0} & 4.3 & 4.2 \\
Badge    & 5.2 $\pm$ {\small 0.01} & 6.3 $\pm$ {\small 0.01} & 2.9 $\pm$ {\small 0.01} & 5.2 $\pm$ {\small 0.01} & 4.7 $\pm$ {\small 0.0}  & 3.3 $\pm$ {\small 0.01} & 3.5 $\pm$ {\small 0.0} & 4.5 & 5.4 \\
LeastConf & 9.2 $\pm$ {\small 0.02} & 10.3 $\pm$ {\small 0.02} & 8.1 $\pm$ {\small 0.02} & 2.1 $\pm$ {\small 0.01} & 4.0 $\pm$ {\small 0.0}  & 7.9 $\pm$ {\small 0.02} & 3.0 $\pm$ {\small 0.01} & 6.4 & 6.5 \\
DSA      & 7.4 $\pm$ {\small 0.02} & 7.3 $\pm$ {\small 0.01} & 7.5 $\pm$ {\small 0.01} & 5.4 $\pm$ {\small 0.01} & 5.1 $\pm$ {\small 0.0}  & 6.0 $\pm$ {\small 0.02} & 7.3 $\pm$ {\small 0.01} & 6.6 & 6.7 \\
BALD     & 4.0 $\pm$ {\small 0.01} & 4.7 $\pm$ {\small 0.01} & 5.4 $\pm$ {\small 0.01} & 12.0 $\pm$ {\small 0.01} & 7.6 $\pm$ {\small 0.0}  & 7.6 $\pm$ {\small 0.02} & 5.0 $\pm$ {\small 0.0} & 6.6 & 7.6 \\
CoreGCN  & 6.9 $\pm$ {\small 0.01} & 4.9 $\pm$ {\small 0.01} & 10.4 $\pm$ {\small 0.01} & 7.6 $\pm$ {\small 0.01} & 6.5 $\pm$ {\small 0.01} & 4.0 $\pm$ {\small 0.01} & 6.8 $\pm$ {\small 0.0} & 6.7 & 8.2 \\
Entropy  & 6.6 $\pm$ {\small 0.02} & 3.9 $\pm$ {\small 0.01} & 7.6 $\pm$ {\small 0.01} & 7.6 $\pm$ {\small 0.01} & 4.9 $\pm$ {\small 0.01} & 9.8 $\pm$ {\small 0.02} & 9.6 $\pm$ {\small 0.0} & 7.1 & 6.5 \\
LSA      & 6.1 $\pm$ {\small 0.01} & 6.8 $\pm$ {\small 0.01} & 5.3 $\pm$ {\small 0.01} & 7.7 $\pm$ {\small 0.01} & 10.6 $\pm$ {\small 0.01} & 7.5 $\pm$ {\small 0.01} & 7.3 $\pm$ {\small 0.01} & 7.3 & 7.5 \\
Random   & 9.0 $\pm$ {\small 0.01} & 9.3 $\pm$ {\small 0.01} & 5.3 $\pm$ {\small 0.01} & 8.4 $\pm$ {\small 0.01} & 11.1 $\pm$ {\small 0.0}  & 7.9 $\pm$ {\small 0.01} & 8.0 $\pm$ {\small 0.0} & 8.4 & 6.9 \\
Coreset  & 7.1 $\pm$ {\small 0.01} & 9.0 $\pm$ {\small 0.01} & 10.5 $\pm$ {\small 0.01} & 6.8 $\pm$ {\small 0.01} & 7.1 $\pm$ {\small 0.0}  & 8.5 $\pm$ {\small 0.02} & 10.8 $\pm$ {\small 0.01} & 8.5 & 7.2 \\
TypiClust & 8.8 $\pm$ {\small 0.01} & 10.2 $\pm$ {\small 0.01} & 12.0 $\pm$ {\small 0.02} & 7.9 $\pm$ {\small 0.01} & 11.0 $\pm$ {\small 0.01} & 12.0 $\pm$ {\small 0.02} & 12.0 $\pm$ {\small 0.01} & 10.5 & 9.2 \\
\end{tabular}
       	}
\end{table}
%
\begin{figure}
    \centering
    \caption{Ranks of each AL method aggregated by domain. Horizontal bars indicate a \textbf{non}-significant rank difference. The significance is tested via a paired-t-test with $\alpha=0.05$.}
    \label{fig:ranks_by_domain}
    \includegraphics[width=0.49\linewidth]{img/macro_vector.jpg}
    \includegraphics[width=0.49\linewidth]{img/macro_text.jpg}
    \includegraphics[width=0.49\linewidth]{img/macro_img.jpg}
    \includegraphics[width=0.49\linewidth]{img/macro_enc.jpg}
    \vspace{-5mm}
\end{figure}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Honeypot and Diverging Sine}
AL approaches can be categorized into two types: uncertainty and geometric approaches.
Typical members of the first category are variants of uncertainty sampling like entropy, margin and LC sampling \cite{wang2014new} as well as BALD \cite{gal2017deep}.
Typical members of the second category are clustering approaches like Coreset \cite{sener2017active}, BADGE \cite{ashdeep} and TypiClust \cite{hacohen2022active}.
Both types of methods have principled shortcomings in terms of their utilized information that makes them unsuitable for certain data distributions. 
To test for these specific shortcomings, we created two synthetic datasets, namely ``Honeypot" and ``Diverging Sine", that are hard to solve for methods focused on the classifier's decision boundary or data clustering respectively. 
To avoid methods memorizing these datasets, they are generated from scratch for each experiment. \\
\begin{figure}[]
	\centering
	\caption{Synthetic ``Honeypot" and ``Diverging Sine" datasets. The optimal decision boundary is not part of the dataset and serves only as a visual guide.}
	\label{fig:synth_results}
	\includegraphics[width=0.4\linewidth]{img/honeypot.jpg}
	\hspace{10mm}
	\includegraphics[width=0.4\linewidth]{img/diverging_sin.jpg}
	\includegraphics[width=0.49\linewidth]{img/micro_honeypot.jpg}
	\includegraphics[width=0.49\linewidth]{img/micro_diverging_sin.jpg}
	\vspace{-5mm}
\end{figure}
%
Honeypot creates two easy to distinguish clusters and one ``honeypot" that represents a noisy region of the dataset with potentially miss-labeled, miss-measured or generally adverse samples.
%This honeypot contains 150 samples of each class, creating a balance of 50\% beneficial samples and 50\% adverse samples in the dataset.
The honeypot is located on the likely decision boundary of a classifier that is trained on the beneficial samples to maximize its adverse impact on purely uncertainty-based AL methods.
Diverging Sine samples datapoints for each class from two diverging sinusoidal functions that are originating from the same y-intercept.
This creates a challenging region on the left hand side, where a lot of datapoints need to be sampled, and an easy region on the right hand side, where very few datapoints are sufficient. 
The repeating nature of a sine function encourages diversity-based AL methods to equally sample the entire length, drastically oversampling the right hand side of the dataset. \\
Both datasets have a budget of $B=60$ and are tested with query sizes 1 and 5. \\[1mm]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\subsection{Results}
We provide the rank of all AL Methods on Honeypot and Diverging Sine in Fig. \ref{fig:synth_results}.
Results for the Honeypot dataset reveal expected shortcomings of uncertainty sampling methods like margin, entropy and LC sampling as well as BALD.
In addition, BADGE is underperforming for this dataset compared to real-world data. 
Both margin sampling and BADGE (the two best methods) being vulnerable to adverse samples or simply measurement noise, highlights the need for further research into robust AL methods. \\
%
Results for Diverging Sine also confirm expected behavior, as clustering methods (Coreset, TypiClust) fall behind uncertainty methods (entropy, margin, LC sampling), with the exception of BADGE. 
The fact that BADGE is able to perform well on Diverging Sine highlights the importance of embeddings for the clustering methods, as the gradient embedding from BADGE seems to be able to encode uncertainty information, guiding the selection into the left hand regions of the dataset. 
We provide a small ablation study on the importance of the embeddings by testing a version of Coreset and TypiClust on this dataset that does not use the embeddings produced by the classification model, but rather clusters the data directly.
``Coreset Raw" and ``TypiClust Raw" both perform worse than their embedding-based counterpart.

\section{Conclusion}
We strongly advocate to test newly proposed AL methods not only on a wide variety of real data domains, but also to pay close attention to the Honeypot and Diverging Sine datasets to reveal principled shortcomings of the method in question.
Both tasks can be easily carried out by implementing the new AL method into our code base.
For Limitations and Future Work, please refer to App. \ref{app:future_work}.


% \newpage
% \paragraph{Acknowledgement}
% anonymous
%TODO
%Funded by the Lower Saxony Ministry of Science and Culture under grant number ZN3492 within the Lower Saxony “Vorab“ of the Volkswagen Foundation and supported by the Center for Digital Innovations (ZDIN).



\bibliographystyle{plain}
\bibliography{al_benchmark_paper.bib} 

\newpage
\section*{NeurIPS Checklist}
\begin{enumerate}
	
	\item For all authors...
	\begin{enumerate}
		\item Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?
		\answerYes{See list of contributions in the Sec. \ref{sec:introduction}.}
		\item Did you describe the limitations of your work?
		\answerYes{We have prepared a Limitations and Future Work section in App. \ref{app:future_work}}
		\item Did you discuss any potential negative societal impacts of your work?
		\answerNo{We strongly believe that there will be no negative impact of our work, as we have only used publicly available datasets, models and methods}
		\item Have you read the ethics review guidelines and ensured that your paper conforms to them?
		\answerYes{}
	\end{enumerate}
	
	\item If you are including theoretical results...
	\begin{enumerate}
		\item Did you state the full set of assumptions of all theoretical results?
		\answerNo{We do not provide theoretical results.}
		\item Did you include complete proofs of all theoretical results?
		\answerNo{We do not provide theoretical results.}
	\end{enumerate}
	
	\item If you ran experiments (e.g. for benchmarks)...
	\begin{enumerate}
		\item Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)?
		\answerYes{We provide a GitHub link in Sec. \ref{sec:introduction}.}
		\item Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)?
		\answerYes{All Hyperparameters can be found in App. \ref{app:hyperparameters}}
		\item Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)?
		\answerYes{All tables provide standard deviations}
		\item Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)?
		\answerYes{Please refer to App. \ref{app:compute}}
	\end{enumerate}
	
	\item If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
	\begin{enumerate}
		\item If your work uses existing assets, did you cite the creators?
		\answerYes{}
		\item Did you mention the license of the assets?
		\answerNA{Licences are public.}
		\item Did you include any new assets either in the supplemental material or as a URL?
		\answerNo{}
		\item Did you discuss whether and how consent was obtained from people whose data you're using/curating?
		\answerNA{All datasets are public.}
		\item Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content?
		\answerNo{The data does not contain personal information.}
	\end{enumerate}
	
	\item If you used crowdsourcing or conducted research with human subjects...
	\begin{enumerate}
		\item Did you include the full text of instructions given to participants and screenshots, if applicable?
		\answerNo{}
		\item Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable?
		\answerNo{}
		\item Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation?
		\answerNo{}
	\end{enumerate}
	
\end{enumerate}

\appendix
\newpage

%\section{Problem Formulation}\label{app:problem_formulation}
%Given
%\begin{itemize}
%	\item a number $B\in\N$ (called budget),
%	\item two spaces $\X$ and $\Y$,  {\tiny e.g., $\X:=\R^M, \Y:=\R^T$},
%	\item a sample $\D_1,\ldots,\D_N \in (\X\times \Y)^*$ of
%	sequences of pairs $(x,y)$  from an unknown distribution $p$
%	(called datasets),
%	%  {\tiny with $|\D_n|\geq B$ for all $n\in 1{:}N$,}
%	{\tiny with $p(\D)=0$ for $|\D|<B$,}
%	\item a function $\ell:\Y\times\Y\rightarrow\R$ (called loss), and
%	\item a function $\hat y:  (\X\times \Y)^* \times \X^* \rightarrow \Y^\X$
%	(called learning algorithm), \\
%	{\tiny where $\Y^\X$ is the space of all function from $\X$ to $\Y$}
%\end{itemize}
%find a function
%\vspace*{-0.5cm}
%\begin{align*}
%	\Omega: (\X\times \Y)^* \times \X^* &\rightarrow \N
%	\quad\quad \text{\tiny (with $a(\D,X) \leq |X|$)} \\
%	& \text{\tiny where $a(\D,X)$ selects an unlabeled instance from $X$}
%\end{align*}
% which is equivariant in the second argument,

%called acquisition function,
%s.t. the expected loss of a model learned on all predictors plus $B$ sequentially acquired targets
%is minimal:
%\begin{align*}
%	\min\ \EE\   &  \{
%	% \operatorname{avg}\limits_{(x,y)\in\D\test} \ell(y, \hat y(x))
%	\ell(\hat y, \D\test)
%	\mid \D\sim p, (\D\train,\D\test):= \text{split}(\D) \}
%	\\
%	\text{with }
%	\hat y:= & A( (\D_{\train_{n_1}},\ldots,\D_{\train_{n_B}}), \D\train|_{\X})
%	\\ 
%	n_b := & a( (\D_{\train_{n_1}},\ldots,\D_{\train_{n_{b-1}}}), \D\train|_{\X}) ,
%	\quad b\in 1{:}B
%\end{align*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{AL Recommendations from Ji et al.}\label{app:recommendations}
\paragraph{Recommendation 1} Use the backbone architecture with
the community-accepted definition that is best suited
for the dataset at hand and consistently use it across all
experiments. In the image classification domain, we
suggest using ResNet18 for CIFAR-10 and CIFAR-100. \\
$\rightarrow$ We are using Resnet18 for our image datasets and re-purposed the LSTM model from \cite{zhou2021towards} for the text datasets.
For the tabular data, we ran a grid-search (full dataset test accuracy) over MLP architectures.

\paragraph{Recommendation 2} Control the type of optimizer across
methods for comparative evaluations to ensure that the
yield performance difference stems from an active learning method itself. As SGD often generalizes better, we
encourage its use for deep active learning.\\
$\rightarrow$ We searched for the best optimizer per dataset via generalization performance (test accuracy). 

\paragraph{Recommendation 3} Pragmatically fix the learning rate
to 0.1 for SGD on image datasets. While continuous
hyperparameter tuning can improve overall performance,
a fixed learning rate does not change the ranking of AL
methods from a comparative evaluation’s point of view.\\
$\rightarrow$ We found that for smaller datasets, a learning rate of 0.1 was unsuitable. Ji et al. only used large image datasets, so a learning rate of 0.1 was sufficient for them. The learning rate is part of our hyperparameter grid-search.

\paragraph{Recommendation 4} One may use data augmentation if
applied consistently across methods, such that it does
not affect the overall ranking. However, a commonly
accepted baseline is needed, e.g., random horizontal flipping and random cropping for image classification.\\
$\rightarrow$ We did not find a data augmentation technique that could be applied equally on all datasets, so we refrained from it.
The only possibility would be Gaussian noise, but the impact of gaussian noise on pre-encoded is not well-understood.

\paragraph{Recommendation 5} Refine model parameters across AL
batch (“warm starts”) to prevent exhaustive reinitialization and feed initialization of the backbone model’s
weights and the “init sets” with fixed inputs over multiple runs to average out the randomness. Moreover, use
identical seeds for all methods under investigation.\\
$\rightarrow$ We employed a novel seeding strategy to closely control the seeding of our experiments (Details in App. \ref{app:seeding_strategy}). We applied warm-starts for most datasets, except pre-encoded ones, because we found a generally better performance, if we train the classifier from scratch. 

\paragraph{Recommendation 6} Run experiments multiple times to
compensate for non-deterministic operations. If the resulting variance is larger than the gained improvement,
use deterministic operations stringently\\
$\rightarrow$ A study on (non-)deterministic operations has not been conducted in this work, but our large number of repetitions compensate for that.

\paragraph{Recommendation 7SW} Configure and verify influence parameter in active learning implementations thoroughly. To foster future research, we
provide implementations as part of our framework at:
https://intellisec.de/research/eval-al\\
$\rightarrow$ We provide our own baseline code, since we implemented a novel seeding strategy and unify many additional data domains in the code.

\paragraph{Recommendation 7HW} Ensure that comparative evaluations are run on identical hardware. While it is not
necessary to execute all experiments on the same physical device, the GPU model, for instance, should be the
same. Do not mix hardware and list hardware details.\\
$\rightarrow$ The large computational cost of our benchmark did not allow us to compute on only one type of hardware (Which would mean to only use part of our cluster). However, our large number of repetitions compensate for that.

\paragraph{Recommendation 8} Consider multiple query-batch sizes
in the evaluation. The choice of the sizes needs to be
appropriate for the total number of unlabeled samples.\\
$\rightarrow$ We employed a wide range of batch sizes. For details, please refer to Table \ref{tab:batch_sizes}.

\paragraph{Recommendation 9} Compare active learning strategies
without sub-sampling, unless one of the approaches uses
it as a fundamental building block. In this case a detailed
analysis of the influence of sub-sampling is necessary.\\
$\rightarrow$ We only carefully employed sub-sampling, when it was absolutely necessary to keep the computation times feasible.

\paragraph{Recommendation 10} Evaluate active learning strategies
on multiple benchmark datasets, that comprise balanced,
imbalanced, small-scale, and large-scale datasets to cover
most relevant cases in practice.\\
$\rightarrow$ We deferred the study on (im-)balanced datasets to future work. However, our benchmark contains datasets of many different sizes and we extend this argument to domains as well.

\paragraph{Recommendation 11} For a comprehensive analysis of
AL strategies, the overall comparative evaluation should
incorporate as many variables from Section 3 to yield a
summarized PPM that is as expressive as possible. \\
$\rightarrow$ In our main result (Tab. \ref{tab:results}) we average the performance over runs, query sizes and datasets.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{AL Pitfalls from Lueth et al.}\label{app:pitfalls}
\paragraph{P1 Data distribution} The proposed evaluation over a diverse selection of dataset distributions
including specific roll-out datasets proved essential for realistic evaluation of QMs as well as the
different training strategies. One main insight is the fact that class distribution is a crucial predictor
for the potential performance gains of AL on a dataset: Performance gains of AL are generally higher
on imbalanced datasets and occur consistently even for ST models with a small starting budget,
which are typically prone to experience cold start problems. This observation is consistent with a few
previous studies [...].\\
$\rightarrow$ We selected our datasets according to their ``potential" for AL. We measured this potential by the distance of most AL methods to random and the distance of the best AL method to our oracle. If both distances are >0, we consider the dataset useful.


\paragraph{P2 Starting budget} The comprehensive study of various starting budgets on all datasets reveals
that AL methods are more robust with regard to small starting budgets than previously reported
[6, 20, 43]. With the exception of Entropy we did not observe cold start problems even for any QM
even in combination with notoriously prone ST models. The described robustness is presumably
enabled by our thorough classifier configuration (P4) and heuristically adapted query sizes (P3). This
finding has great impact potential suggesting that AL can be applied at earlier points in the annotation
process thereby further reducing the labeling cost [...].\\
$\rightarrow$ Our experiments also showed a high resistance against the cold-start problem, which prompted us to use the smallest possible seed set for most datasets (1 point per class), with the only exception being (un-)encoded Cifar10 and FashionMnist. Here we employ a seed set of 100 points per class to avoid a cold-start.

\paragraph{P3 Query size} Based on our evaluation of the query size we can empirically confirm its importance
with regard to 1) general AL performance and 2) counteracting the cold start problem. The are,
however, surprising findings indicating that the exact interaction between query size and performance
remains an open research question. [...].\\
$\rightarrow$ We generally observed a decreasing performance for larger query sizes. We therefore made sure, that we include the smallest possible query sizes that result in feasible computation times.

\paragraph{P4 Classifer configuration} Our results show that method configuration on a properly sized validation
set is essential for realistic evaluation in AL. [...] This raises the question of to which extent reported
AL advantages could have been achieved by simple classifier configurations. Further, our models
also generally outperform expensively configured models by Munjal et al.. Thus, we conclude
that manually constraining the search space renders HP optimization feasible in practice without
decreasing performance and ensures performance gains by Active Learning are not overstated. The
importance of the proposed strategy to optimize HPs on the starting budget for each new dataset is
supported by the fact that the resulting configurations change across datasets.\\
$\rightarrow$ We also strongly advocate the use of a fully fledged validation set for HP tuning, as this allows for a higher quality of HPs, which in turn reduces the variance of the experiments.

\paragraph{P5 Alternative training paradigms} Based on our study benchmarking AL in the context of
both Self-SL and Semi-SL, we see that while Self-SL generally leads to improvements across all
experiments, Semi-SL only leads to considerably improved performance on the simpler datasets
CIFAR-10/100, on which Semi-SL methods are typically developed. Generally, models trained
with either of the two training paradigms receive a lower performance gain from AL (over random
querying) compared to ST. [...] The fact that AL entails multiple training iterations
amplifies the computational burden of Semi-SL, rendering their combination prohibitively expensive
in most practical scenarios. Further, the fact that our Semi-SL models based on Fixmatch do not seem
to generalize to more complex datasets in our setting stands in stark contrast to conclusions drawn by
[...] as to which the emergence of Semi-SL renders AL redundant. Interestingly, the exact settings
where Semi-SL does not provide benefits in our study are the ones where AL proved advantageous.
The described contradiction with the literature underlines the importance of our proposed protocol
testing for a method’s generalizability to unseen datasets. \\
$\rightarrow$ Due to the high computational costs described by Lueth et al., we opted for the most efficient form of semi-supervised learning, which is to train a fixed encoder-model, pre-encode the datasets and then only train a single linear layer as classifier.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Difference of Ranks with 3 Repetitions}\label{app:rank_difference}
Table \ref{tab:rank_diff_1} and Table \ref{tab:rank_diff_2} follow the exact same computation of ranks that created the main result (Table \ref{tab:results}) with the only difference being a reduced number of runs per AL method.
For each table we sampled 3 runs uniformly at random from the available 50 per AL method. \\
We can observe significant differences between the two tables: \\
{\color{purple}Purple}: A multitude of rank differences of AL methods for specific datasets, some as high as 4.7 ranks for TypiClust on the Splice dataset \\
{\color{olive}Olive}: Well separated AL methods in Tab. \ref{tab:rank_diff_2} (Margin and BADGE) are almost indistinguishable in Tab \ref{tab:rank_diff_1} \\
{\color{red}Red}: BALD lost 2 places in the overall ranking and Entropy gained 2 \\ [1mm]
Even though the overall ordering of AL methods stayed relatively unchanged due to the averaging across many datasets, each individual dataset was subject to drastic permutations.
This highlights the need for many repetitions in AL experiments.
\begin{table}[H]
	\centering
	\caption{Ranks of all AL methods per dataset. First random draw of 3 runs from the overall pool of 50.}
	\label{tab:rank_diff_1}
	\resizebox{\columnwidth}{!}{
		\begin{tabular}{l|lllllll|ll}
			& Splice & DNA  & USPS & Cifar10 & FMnist & TopV2 & News & Unencoded & Encoded \\
			\hline
			Oracle         & 1.0    & 1.0  & 1.0  & 1.0     & 1.0          & 1.0   & 1.0  & 1.0       & 2.1     \\
			Margin         & 6.0    & 7.3  & 2.0  & 6.7     & 5.3          & 2.3   & 3.3  & {\color{olive}4.7}& 4.4     \\
			Badge          & 6.0    & 7.3  & 3.0  & 6.7     & 5.0          & 3.3   & 4.0  & {\color{olive}5.0}& 5.3     \\
			{\color{red}BALD}& 3.3    & 4.7  & 5.3  & 12.0    & 7.0          & 6.3   & 4.3  & 6.1       & 7.9     \\
			CoreGCN        & 8.7    & 3.7  & 10.7 & 6.3     & 5.3          & 4.0   & 7.7  & 6.6       & 9.1     \\
			DSA            & 8.3    & 6.3  & 7.7  & 7.7     & 4.3          & 6.7   & 6.7  & 6.8       & 6.1     \\
			LeastConf      & 10.0   & 12.0 & 8.0  & 3.0     & 4.3          & 9.3   & 2.3  & 7.0       & 6.7     \\
			LSA            & 5.7    & 6.7  & 5.3  & 6.7     & 10.7         & 7.7   & 7.0  & 7.1       & 6.3     \\
			{\color{red}Entropy}        & 11.0   & 3.3  & 7.3  & 4.0     & 6.7          & 8.3   & 9.7  & 7.2       & 7.0     \\
			Random         & 7.7    & 8.7  & 5.3  & 8.0     & 11.0         & 8.0   & 9.0  & 8.2       & 6.3     \\
			Coreset        & 4.7    & 10.3 & 10.3 & 7.7     & 6.0          & 9.0   & 11.0 & 8.4       & 7.2     \\
			TypiClust      & {\color{purple}5.7}& 6.7  & 12.0 & 8.3     & 11.3         & 12.0  & 12.0 & 9.7       & 9.7    
		\end{tabular}
	}
\end{table}
%
\begin{table}[H]
	\centering
	\caption{Ranks of all AL methods per dataset. Second random draw of 3 runs from the overall pool of 50.}
	\label{tab:rank_diff_2}
	\resizebox{\columnwidth}{!}{
		\begin{tabular}{l|lllllll|ll}
			& Splice & DNA  & USPS & Cifar10 & FMnist & TopV2 & News & Unencoded & Encoded \\
			\hline
			Oracle         & 1.0    & 1.0  & 1.0  & 1.0     & 1.0          & 1.0   & 1.0  & 1.0       & 2.4     \\
			Margin         & 6.0    & 3.3  & 2.0  & 5.7     & 2.0          & 2.0   & 4.3  & {\color{olive}3.6}& 3.8     \\
			Badge          & 6.0    & 9.0  & 3.0  & 3.0     & 5.7          & 3.7   & 3.3  & {\color{olive}4.8}& 4.9     \\
			CoreGCN        & 4.3    & 6.3  & 10.3 & 7.3     & 5.3          & 5.7   & 5.3  & 6.4       & 8.1     \\
			DSA            & 8.7    & 7.3  & 7.3  & 6.0     & 4.3          & 5.3   & 6.0  & 6.4       & 6.5     \\
			{\color{red}BALD}& 4.7    & 4.0  & 4.7  & 12.0    & 7.3          & 6.7   & 6.7  & 6.6       & 7.5     \\
			{\color{red}Entropy}        & 6.7    & 4.7  & 7.7  & 5.3     & 5.0          & 7.3   & 9.3  & 6.6       & 6.8     \\
			LeastConf      & 7.7    & 10.0 & 8.3  & 3.3     & 6.0          & 8.7   & 3.0  & 6.7       & 7.3     \\
			LSA            & 7.7    & 5.3  & 6.0  & 9.0     & 11.0         & 9.0   & 7.3  & 7.9       & 7.5     \\
			Random         & 9.3    & 8.0  & 5.0  & 8.7     & 11.7         & 8.3   & 8.7  & 8.5       & 7.6     \\
			Coreset        & 6.0    & 10.7 & 10.7 & 8.0     & 8.3          & 8.3   & 11.0 & 9.0       & 6.3     \\
			TypiClust      & {\color{purple}10.0}& 8.3  & 12.0 & 8.7     & 10.3         & 12.0  & 12.0 & 10.5      & 9.4    
		\end{tabular}
	}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
\section{Seeding Strategy}\label{app:seeding_strategy}
We aim to provide an experimental setup that is fully reproducible independent of the dataset, classification model, or AL method used.
For a fair comparison of two AL methods, both methods need to receive equal starting conditions in terms of train/validation split, initialization of classifier, and even the state of minor systems like the optimizer or mini-batch sampler.
Even though different implementations might have their own solution to some of these problems, only \cite{ji2023randomness} has described and implemented a fully reproducible pipeline for AL evaluation.
The term reproducibility in this work is used as a synonym not only for the reproducibility of an experiment (a final result given a seed), but also the reproducibility of all subsystems independent of each other.
The seed for one subsystem should always reproduce the behavior of this subsystem independent of all other subsystems and their seeds.
The main obstacle for ensuring reproducibility is the seeding utility in PyTorch, Tensorflow, and other frameworks, whose default choice is a single global seed.
Since many subsystems draw random numbers from this seed, all of them influence each other to a point where a single additional draw can completely change the model initialization, data split or the order of training batches.
Even though some workarounds exist, e.g. re-setting the seed multiple times, this problem is not limited to the initialization phase, but also extends to the AL iterations and the systems within.
We propose an implementation that creates separate Random Number Generators (RNGs) for each of these systems to ensure equal testing conditions even when the AL method, dataset, or classifier changes.
We hypothesize that the insufficient setup with global seeds contributes to the ongoing problem of inconsistent results of AL methods in different papers. \\ [1mm]
In summary, we introduce three different seeds: $s_\Omega$ for the AL method, $s_\mathcal{D}$ for dataset splitting and mini-batch sampling, and $s_\theta$ for model initialization and sampling of dropout masks.
Unless stated otherwise, we will keep $s_\Omega$ fixed, while $s_\mathcal{D}$ and $s_\theta$ are incremented by 1 between repetitions to introduce stochasticity into our framework.
Some methods require a subsample to be drawn from $\mathcal{U}$ in order to reduce the computational cost in each iteration, while others need access to the full unlabeled pool (e.g. for effective clustering).
If a subsample is required, it will be drawn from $s_\Omega$ and therefore will not influence other systems in the experiments.
For each method, we decided if subsampling is required based on our available hardware, but decided against setting a fixed time limit per experiment, since this would introduce unnecessary complexity into the benchmark.
An overview of selected hyperparameters per AL method can be found in Appendix \ref{app:agent_hyperparameters}. \\
\textbf{Note:} Even though we decoupled the subsystems via the described seeds, the subsystems can still influence each other in a practical sense. 
For example, keeping $s_\mathcal{D}$ fixed does not mean that always the same sequence of samples from $\mathcal{U}$ (if subsamples are drawn) are shown to all AL methods. 
This is practically impossible, as different AL methods pick different $x^{(i)}$.
However, the hypothetical \textbf{tree} of all possible sequences of samples from $\mathcal{U}$ remains the same, granting every AL methods equal possibilities.


\section{Hyperparameters and Preprocessing per Dataset}\label{app:hyperparameters}
For all our datasets we use the pre-defined train/test splits, if given. 
In the remaining cases, we define test sets upfront and store them into separate files to keep them fixed across all experiments.
The validation set is split in the experiment run itself and depends on the dataset-seed.\\
\textbf{Tabular:}
We use \textbf{Splice}, \textbf{DNA} and \textbf{USPS} from LibSVMTools \cite{libsvmtools}.
All three datasets are normalized between [0, 1]. \\
\textbf{Image:}
We use \textbf{FashionMNIST} \cite{xiao2017fashion} and \textbf{Cifar10} \cite{krizhevsky2009learning}, since both are widely used in AL literature.
Both datasets are normalized according to their standard protocols. \\
\textbf{Text:}
We use \textbf{News Category} \cite{misra2022news} and \textbf{TopV2} \cite{chen-etal-2020-low-resource}.
For News Category we use  the 15 most common categories as indicated by its Kaggle site.
We additionally drop sentences above 80 words to reduce the padding needed (retaining 99,86\% of the data).
For TopV2, we are only using the "alarm" domain.
Both datasets are encoded with pre-trained GloVe (Common Crawl 840B Tokens) embeddings \cite{pennington2014glove}.
Since neither dataset provided a fixed test set, we randomly split 7000 datapoints into a test set.
%
\begin{table}[H]
	\centering
	\begin{tabular}{l || l l l }
		Dataset & Seed Set & Budget & Val Split \\
		\hline
		Splice & 1 & 400 & 0.2 \\
		SpliceEnc. & 1 & 60 & 0.2 \\
		DNA & 1 & 300 & 0.2 \\
		DNAEnc & 1 & 40 & 0.2 \\
		USPS & 1 & 400 & 0.2 \\
		USPSEnc & 1 & 600 & 0.2 \\
		FashionMnist & 100 & 2000 & 0.04 \\
		FashionMnistEnc & 1 & 500 & 0.04 \\
		Cifar10 & 100 & 2000 & 0.04  \\
		Cifar10Enc & 1 & 350 & 0.04  \\
		TopV2 & 1 & 125 & 0.25  \\
		News & 1 & 1500 & 0.03  \\
	\end{tabular}
	\caption{Size of the seed set is given by number of labeled sample per class.}
	\label{tab:architecture_hps}
\end{table}
\begin{table}[H]
	\centering
	\begin{tabular}{l || l | l l l l l}
		Dataset & Classifier & Optimizer & LR & Weight Decay & Dropout & Batch Size \\
		\hline
		Splice & [24, 12] & NAdam & 1.2e-3 & 5.9e-5 & 0 & 43 \\
		SpliceEnc. & linear & NAdam & 6.2e-4 & 5.9e-6 & 0 & 64 \\
		DNA & [24, 12] & NAdam & 3.9e-2 & 3.6e-5 & 0 & 64 \\
		DNAEnc & linear & NAdam & 1.6e-3 & 4e-4 & 0 & 64 \\
		USPS & [24, 12] & Adam & 8.1e-3 & 1.5e-6 & 0 & 43 \\
		USPSEnc & linear & NAdam & 7.8e-3 & 1.9e-6 & 0 & 64 \\
		FashionMnist & ResNet18 & NAdam & 1e-3 & 0 & 0 & 64 \\
		FashionMnistEnc & linear & Adam & 1.6e-3 & 1e-5 & 5e-2 & 64 \\
		Cifar10 & ResNet18 & NAdam & 1e-3 & 0 & 0 & 64 \\
		Cifar10Enc & linear & NAdam & 1.7e-3 & 2.3e-5 & 0 & 64 \\
		TopV2 & BiLSTM & NAdam & 1.5e-3 & 1.7e-7 & 5e-2 & 64 \\
		News & BiLSTM & NAdam & 1.5e-3 & 1.7e-7 & 5e-2 & 64 \\
	\end{tabular}
	\caption{Classifier architectures and optimized hyperparameters per dataset. Numbers in brackets signify a MLP with corresponding hidden layers.}
	\label{tab:architectures}
\end{table}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Performance Curves per Dataset}\label{app:all_results}
% Splice
\begin{figure}[H]
	\centering
	\caption{Performance curves per query size for normal (un-encoded) Splice}
	\includegraphics[width=\linewidth]{img/eval_splice}
\end{figure}
%Splice Enc
\begin{figure}[H]
	\centering
	\caption{Performance curves per query size for semi-supervised (encoded) Splice}
	\includegraphics[width=\linewidth]{img/eval_splice_enc}
\end{figure}
%DNA
\begin{figure}[H]
	\centering
	\caption{Performance curves per query size for normal (un-encoded) DNA}
	\includegraphics[width=\linewidth]{img/eval_dna}
\end{figure}
%DNA Enc
\begin{figure}[H]
	\centering
	\caption{Performance curves per query size for semi-supervised (encoded) DNA}
	\includegraphics[width=\linewidth]{img/eval_dna_enc} 
\end{figure}
%USPS
\begin{figure}[H]
	\centering
	\caption{Performance curves per query size for normal (un-encoded) USPS}
	\includegraphics[width=\linewidth]{img/eval_usps}
\end{figure}
%USPS Enc
\begin{figure}[H]
	\centering
	\caption{Performance curves per query size for semi-supervised (encoded) USPS}
	\includegraphics[width=\linewidth]{img/eval_usps_enc}
\end{figure}
%Cifar10
\begin{figure}[H]
	\centering
	\caption{Performance curves per query size for normal (un-encoded) Cifar10}
	\includegraphics[width=\linewidth]{img/eval_cifar10}
\end{figure}
%Cifar10 Encoded
\begin{figure}[H]
	\centering
	\caption{Performance curves per query size for semi-supervised (encoded) Cifar10}
	\includegraphics[width=\linewidth]{img/eval_cifar10_enc}
\end{figure}
%FMnist
\begin{figure}[H]
	\centering
	\caption{Performance curves per query size for normal (un-encoded) FashionMnist}
	\includegraphics[width=\linewidth]{img/eval_fmnist}
\end{figure}
%FashionMnist Encoded
\begin{figure}[H]
	\centering
	\caption{Performance curves per query size for semi-supervised (encoded) FashionMnist}
	\includegraphics[width=\linewidth]{img/eval_fmnist_enc}
\end{figure}
%TopV2
\begin{figure}[H]
	\centering
	\caption{Performance curves per query size for normal (GloVe) TopV2}
	\includegraphics[width=\linewidth]{img/eval_topv2}
\end{figure}
%News
\begin{figure}[H]
	\centering
	\caption{Performance curves per query size for normal (GloVe) News}
	\includegraphics[width=\linewidth]{img/eval_news}
\end{figure}
%ThreeClust
\begin{figure}[H]
	\centering
	\caption{Performance curves per query size for normal (un-encoded) Honeypot}
	\includegraphics[width=\linewidth]{img/eval_honeypot}
\end{figure}
%DivergingSin
\begin{figure}[H]
	\centering
	\caption{Performance curves per query size for normal (un-encoded) Diverging Sine}
	\includegraphics[width=\linewidth]{img/eval_diverging_sine}
\end{figure}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Critical Difference Diagrams}\label{app:cd_diagrams}
We adapted the code for the CD diagrams from \cite{cd_diagram_code}. \\
To compare each AL method, we consider each combination of dataset, query size and run is considered a separate ``experiment", i.e. the results of \verb|Dataset1-QuerySize1-run5| of an AL Method \verb|x| is only compared to the results of \verb|Dataset1-QuerySize1-run5| of AL method \verb|y|. \\
Depending on the use-case, we build the following ``experiments":
\begin{itemize}
	\item Single dataset - single query size: Each AL method has 50 ``experiments" in it's 50 repetitions
	\item Single dataset - all query sizes (Fig. \ref{fig:synth_results}): Each ``experiment" is represented by a string \verb|query_size_<qs>_run_<id>|
	\item Multiple dataset - all query sizes (Fig. \ref{fig:ranks_by_domain}): Each ``experiment" is represented by a string \verb|dataset_<dataset>_query_size_<qs>_run_<id>|
\end{itemize}
Due to the large number of restarts and the wide range of datasets and query sizes, we can provide very accurate significance tests.



\section{AL Pseudocode}\label{app:pseudocode}
\begin{algorithm}[H]
	\caption{Active Learning Loop}\label{alg:active_learning}
	\begin{algorithmic}[1]
		\Require $\LL, \U, \D_\text{test}, \operatorname{Train}, \operatorname{Seed}, \hat y$
		\Require $\Omega$ \Comment{AL Method}
		\State $\LL^{(1)} \gets \operatorname{Seed}(\U)$  \Comment{Create the initial labeled set}
		\State $\U^{(1)} \gets \U$
		\For{$i := 1 \ldots B$}
		\State $\text{acc}^{(i)} \gets \operatorname{Train}(\LL^{(i)})$ 
		\State $a^{(i)} \gets \Omega(\mathcal{U}^{(i)})$ 
		\State $\mathcal{L}^{(i+1)} \gets \mathcal{L}^{(i)} \cup \{(\U^{(i)}_a, A(\U^{(i)}_{a}))\}$
		\State $\U^{(i+1)} \gets \U^{(i)} \setminus \{\U^{(i)}_a\}$
		\EndFor
		\State
		\Return $\frac{1}{B} \sum_{i=1}^{B} \text{acc}^{(i)}$
	\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
	\caption{Retrain}\label{alg:retrain}
	\begin{algorithmic}[1]
		\Require $\LL, \D_\text{val}, \D_\text{test}$
		\Require $\hat y, e_\text{max}$
		\State $\text{loss}^* \gets \infty$
		\For{$i := 1 \ldots e^{\text{max}}$}
		\State $\hat y_{i+1} \gets \hat y_i - \eta \nabla_{\hat y} \ell(\mathcal{L}, \hat y)$
		\State $\text{loss}_i \gets \ell(\mathcal{D}^\text{val}, \hat y)$
		\If{$\text{loss}_i < \text{loss}^*$}
		\State $\text{loss}^* \gets \text{loss}_i$
		\Else
		\State Break
		\EndIf
		\EndFor
		\State
		\Return Acc($\mathcal{D}^\text{test}, \hat y$)
	\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
	\caption{Acquire Oracle $\Omega$}\label{alg:oracle}
	\begin{algorithmic}[1]
		\Require $\mathcal{U}, \mathcal{L}, A, \mathcal{D}_\text{test}, \tau, \hat y_\theta$ 
		\Require Train, Margin, Acc
		\State $\text{acc}^0 \gets \text{acc}^* \gets \operatorname{Acc}(\mathcal{D}_\text{test}, \hat y_\theta)$ 
		\For{$k := 1 \ldots \tau$}
		\State $u_k = \operatorname{unif}(\U)$
		\State $\mathcal{L}' \gets \mathcal{L}^{(i)} \cup \{(u_k, A(u_k))\}$
		\State $\hat y'_\theta \gets \operatorname{Train}(\mathcal{L}', \hat y_\theta)$
		\State $\text{acc}' \gets \text{Acc}(\D_\test, \hat y'_\theta)$
		%\State $r \gets \text{acc}' - \text{acc}$
		\If{$\text{acc}' > \text{acc}^*$} 
		\State $\text{acc}^* \gets \text{acc}'$
		\State $u^* \gets u_k$
		\EndIf
		\EndFor
		\If{$\text{acc}^0 = \text{acc}^*$}
		\State $u^* \gets \operatorname{Margin}(\mathcal{U}, \hat y_\theta)$
		\EndIf
		\Return $u^*$
	\end{algorithmic}
\end{algorithm}
Alg. \ref{alg:oracle} replaces the AL method $\Omega$ in the AL loop (Alg. \ref{app:pseudocode} line 5). 



\section{Oracle Curve Forecasting}\label{app:oracle_forecasting}
Unfortunately, the iterative nature of our oracle means that the computational effort scales in the budget $B$.
For datasets with large budgets, like Cifar10 and FashionMnist (both 10K), we were unable to compute the oracle set for the entire 10K iterations. \\
We compromised by (i) picking the two points with highest test accuracy, instead of only one and (ii) only computed until iteration 2000. \\
The rest of the curve was forecast using a simple 2-step algorithm, based on linear regression:
\begin{enumerate}
	\item Fit a linear regression model on the second 50\% of the existing oracle curve (to accurately capture the trend of the oracle, rather than the intercept) and forecast the oracle performance for the remaining budget.
	\item Post-process the oracle forecast by letting it asymptotically approach the upper bound performance of the dataset.
\end{enumerate}
\begin{align}
	o_i &= \operatorname{min} \begin{cases} 
		o_i \\
		\phi(i) * o_i + (1 - \phi(i)) * \text{upper bound} 
	\end{cases} \\
	\phi(i) &= e^{-i / 0.5}
\end{align}
\begin{figure}[H]
	\centering
	\caption{(left) Oracle forecast (dotted line) for FashionMnist with query size 500; (right) function $\phi$ that governs the approach towards the upper bound performance.}
	\includegraphics[width=0.55\linewidth]{img/eval_fmnist_500.pdf}
	\includegraphics[width=0.43\linewidth]{img/phi_example.pdf}
\end{figure}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{AL Methods}\label{app:acquisition_functions}
\textbf{Uncertainty Sampling} 
tries to find the sample that the classifier is most uncertain about by computing heuristics of the class probabilities. For our benchmark, we use entropy and margin (a.k.a. best-vs-second-best) sampling.\\
\textbf{BALD \cite{kirsch2019batchbald}}
applies the query-by-committee strategy of model ensembles to a single model by interpreting the classifier's parameters as distributions and then sample multiple outputs from them via Monte-Carlo dropout.\\
\textbf{BADGE \cite{ashdeep}} uses gradient embeddings of unlabeled points to select samples where the classifier is expected to change a lot. The higher the magnitude of the gradient the higher the expected improvement in model performance.
BADGE employs a variant to the KMeans++ initialization technique to select batches of points. Even though \cite{ashdeep} provided pseudocode for this procedure that selects the first point at random, all found implementations of BADGE select the first points instead by maximum gradient magnitude. \\
\textbf{Coreset \cite{sener2017active}}
employs K-Means clustering trying to cover the whole data distribution.
Selects the unlabeled sample that is the furthest away from all cluster centers.
Clustering is done in a semantically meaningful space by encoding the data with the current classifier $\hat y$.
In this work, we use the greedy variant of Coreset.\\
\textbf{TypiClust \cite{hacohen2022active}}
relies on clustering similar to Coreset, but proposes a new measure called ``Typicality" to select unlabeled samples.
It selects points that are in the densest regions of clusters that do not contain labeled samples yet.
Clustering is done in a semantically meaningful space by encoding the data with the current classifier $\hat y$.
It has to be pointed out that TypiClust was designed for low-budget scenarios, but we think it is still worthwhile to test and compare this method with higher budgets. \\
\textbf{Core-GCN \cite{caramalau2021sequential}} trains a Graph-Convolutional-Network (GCN) on embeddings of the unlabeled pool, obtained from the classifier (Similar to Coreset and TypiClust). 
This GCN model propagates uncertainty information through the graph and therefore enhances the nodes uncertainty quantification. Lastly, the node that displays the highest amount of uncertainty is selected for labeling. \\
\textbf{DSA/LSA \cite{kim2019guiding}} use the metric of test adequacy to construct a set of points that is diverse, ranging from points that are close to points in $\LL$ and points that are significantly different. DSA and LSA measure the diversity of points by distance in embedding space or likelihood estimation under the given classifier respectively. \\ [2mm]
%
\textbf{Excluded Methods}\\
\textbf{Learning Loss for AL \cite{yoo2019learning}}
Introduces an updated training of the classification model with an auxiliary loss and therefore cannot be compared fairly against classification models without this boosted training regime.\\ [1mm]
%
\textbf{Reinforcement Learning Methods} \\
We postpone the study of learned AL methods to future versions of this benchmark, as reinforcement learning is infamous for being extremely time consuming and itself hard to reproduce .


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{AUCs by Query Size}\label{app:auc_by_query_size}
All tables are sorted according to the main result in Table \ref{tab:results}.
% QS 1
\begin{table}[H]
	\caption{AUC values for each dataset that supports query size 1.}
	\resizebox{\columnwidth}{!}{
		\begin{tabular}{l|llllllllllll}
			& Wins & Splice       & SpliceEnc& DNA          & DNAEnc  & USPS          & USPSEnc  & Cifar10Enc& FMnistEnc& TopV2     & Diverging Sine & ThreeClust \\
			\hline
			Oracle &  & 0.803 $\pm$ {\small 0.012} & 0.678 $\pm$ {\small 0.021} & 0.825 $\pm$ {\small 0.009} & 0.721 $\pm$ {\small 0.013} & 0.866 $\pm$ {\small 0.004} & 0.436 $\pm$ {\small 0.057} & 0.749 $\pm$ {\small 0.009} & 0.755 $\pm$ {\small 0.005} & 0.884 $\pm$ {\small 0.006} & 0.957 $\pm$ {\small 0.009} & 0.783 $\pm$ {\small 0.03} \\
			
			Margin & 3 & 0.769 $\pm$ {\small 0.021} & \textbf{0.678 $\pm$ {\small 0.032}} & 0.806 $\pm$ {\small 0.013} & 0.642 $\pm$ {\small 0.047} & \textbf{0.858 $\pm$ {\small 0.006}} & 0.426 $\pm$ {\small 0.038} & 0.653 $\pm$ {\small 0.013} & 0.68 $\pm$ {\small 0.012} & \textbf{0.861 $\pm$ {\small 0.009}} & 0.941 $\pm$ {\small 0.018} & 0.704 $\pm$ {\small 0.074} \\
			
			Badge & 0 & 0.767 $\pm$ {\small 0.02} & 0.661 $\pm$ {\small 0.026} & 0.78 $\pm$ {\small 0.014} & 0.642 $\pm$ {\small 0.046} & 0.83 $\pm$ {\small 0.008} & 0.371 $\pm$ {\small 0.035} & 0.656 $\pm$ {\small 0.013} & 0.68 $\pm$ {\small 0.009} & 0.826 $\pm$ {\small 0.024} & 0.941 $\pm$ {\small 0.017} & 0.69 $\pm$ {\small 0.083} \\
			
			LeastConf & 2 & 0.779 $\pm$ {\small 0.019} & 0.68 $\pm$ {\small 0.032} & 0.809 $\pm$ {\small 0.01} & 0.629 $\pm$ {\small 0.05} & 0.846 $\pm$ {\small 0.009} & 0.421 $\pm$ {\small 0.039} & \textbf{0.668 $\pm$ {\small 0.014}} & \textbf{0.685 $\pm$ {\small 0.009}} & 0.843 $\pm$ {\small 0.013} & 0.94 $\pm$ {\small 0.016} & 0.692 $\pm$ {\small 0.094}\\
			
			DSA & 3 & 0.766 $\pm$ {\small 0.021} & 0.691 $\pm$ {\small 0.022} & 0.803 $\pm$ {\small 0.01} & \textbf{0.646 $\pm$ {\small 0.032}} & 0.829 $\pm$ {\small 0.01} & \textbf{0.431 $\pm$ {\small 0.05}} & 0.663 $\pm$ {\small 0.014} & 0.679 $\pm$ {\small 0.01} & 0.844 $\pm$ {\small 0.017} & 0.941 $\pm$ {\small 0.014} & \textbf{0.731 $\pm$ {\small 0.032}}\\
			
			BALD & 1 & \textbf{0.78 $\pm$ {\small 0.014}} & 0.649 $\pm$ {\small 0.04} & 0.784 $\pm$ {\small 0.01} & 0.632 $\pm$ {\small 0.042} & 0.819 $\pm$ {\small 0.01} & 0.242 $\pm$ {\small 0.046} & 0.666 $\pm$ {\small 0.014} & 0.644 $\pm$ {\small 0.018} & 0.815 $\pm$ {\small 0.024} & 0.928 $\pm$ {\small 0.014} & 0.698 $\pm$ {\small 0.043}\\
			
			CoreGCN & 2 & 0.765 $\pm$ {\small 0.021} & 0.686 $\pm$ {\small 0.023} & 0.804 $\pm$ {\small 0.012} & \textbf{0.646 $\pm$ {\small 0.03}} & 0.753 $\pm$ {\small 0.016} & 0.39 $\pm$ {\small 0.044} & 0.623 $\pm$ {\small 0.018} & 0.647 $\pm$ {\small 0.012} & 0.85 $\pm$ {\small 0.01} & 0.938 $\pm$ {\small 0.014} & \textbf{0.731 $\pm$ {\small 0.028}}\\
			
			Entropy & 3 & 0.768 $\pm$ {\small 0.022} & \textbf{0.678 $\pm$ {\small 0.035}} & \textbf{0.812 $\pm$ {\small 0.013}} & 0.635 $\pm$ {\small 0.045} & 0.83 $\pm$ {\small 0.011} & 0.399 $\pm$ {\small 0.035} & 0.663 $\pm$ {\small 0.013} & 0.681 $\pm$ {\small 0.011} & 0.815 $\pm$ {\small 0.021} & \textbf{0.942 $\pm$ {\small 0.017}} & 0.696 $\pm$ {\small 0.083}\\
			
			LSA & 0 & 0.772 $\pm$ {\small 0.016} & 0.68 $\pm$ {\small 0.026} & 0.787 $\pm$ {\small 0.012} & 0.618 $\pm$ {\small 0.036} & 0.821 $\pm$ {\small 0.009} & 0.422 $\pm$ {\small 0.037} & 0.613 $\pm$ {\small 0.014} & 0.642 $\pm$ {\small 0.012} & 0.816 $\pm$ {\small 0.013} & 0.932 $\pm$ {\small 0.016} & 0.727 $\pm$ {\small 0.033}\\
			
			Random & 0 & 0.76 $\pm$ {\small 0.016} & 0.674 $\pm$ {\small 0.027} & 0.774 $\pm$ {\small 0.013} & 0.63 $\pm$ {\small 0.035} & 0.823 $\pm$ {\small 0.009} & 0.404 $\pm$ {\small 0.036} & 0.613 $\pm$ {\small 0.014} & 0.639 $\pm$ {\small 0.013} & 0.815 $\pm$ {\small 0.012} & 0.933 $\pm$ {\small 0.017} & 0.721 $\pm$ {\small 0.036}\\
			
			Coreset & 0 & 0.772 $\pm$ {\small 0.016} & 0.69 $\pm$ {\small 0.017} & 0.79 $\pm$ {\small 0.012} & 0.638 $\pm$ {\small 0.041} & 0.767 $\pm$ {\small 0.016} & 0.404 $\pm$ {\small 0.046} & 0.659 $\pm$ {\small 0.011} & 0.684 $\pm$ {\small 0.009} & 0.826 $\pm$ {\small 0.022} & 0.937 $\pm$ {\small 0.014} & 0.73 $\pm$ {\small 0.031}\\
			
			TypiClust & 0 & 0.762 $\pm$ {\small 0.016} & 0.685 $\pm$ {\small 0.025} & 0.778 $\pm$ {\small 0.01} & 0.663 $\pm$ {\small 0.028} & 0.828 $\pm$ {\small 0.007} & 0.396 $\pm$ {\small 0.046} & 0.653 $\pm$ {\small 0.013} & 0.649 $\pm$ {\small 0.007} & 0.831 $\pm$ {\small 0.011} & 0.934 $\pm$ {\small 0.018} & 0.727 $\pm$ {\small 0.033}\\
		\end{tabular}
	}
\end{table}
% QS 5
\begin{table}[H]
	\caption{AUC values for each dataset that supports query size 5.}
	\resizebox{\columnwidth}{!}{
		\begin{tabular}{l|llllllllllll}
			& Wins& Splice & SpliceEnc & DNA & DNAEnc & USPS & USPSEnc & Cifar10Enc & FMnistEnc & TopV2 & DivergingSin & ThreeClust \\
			\hline
			Oracle &  & 0.803 $\pm$ {\small 0.012} & 0.678 $\pm$ {\small 0.021} & 0.825 $\pm$ {\small 0.009} & 0.721 $\pm$ {\small 0.013} & 0.866 $\pm$ {\small 0.004} & 0.436 $\pm$ {\small 0.057} & 0.749 $\pm$ {\small 0.009} & 0.755 $\pm$ {\small 0.005} & 0.884 $\pm$ {\small 0.006} & 0.957 $\pm$ {\small 0.009} & 0.783 $\pm$ {\small 0.03} \\
			
			Margin & 3 & 0.765 $\pm$ {\small 0.021} & 0.662 $\pm$ {\small 0.032} & 0.794 $\pm$ {\small 0.011} & 0.611 $\pm$ {\small 0.05} & \textbf{0.855 $\pm$ {\small 0.006}} & \textbf{0.508 $\pm$ {\small 0.02}} & 0.656 $\pm$ {\small 0.014} & 0.678 $\pm$ {\small 0.009} & \textbf{0.848 $\pm$ {\small 0.013}} & 0.923 $\pm$ {\small 0.019} & 0.697 $\pm$ {\small 0.055} \\
			
			Badge & 2 & 0.768 $\pm$ {\small 0.014} & 0.646 $\pm$ {\small 0.035} & 0.785 $\pm$ {\small 0.011} & \textbf{0.624 $\pm$ {\small 0.036}} & 0.846 $\pm$ {\small 0.007} & 0.48 $\pm$ {\small 0.021} & 0.647 $\pm$ {\small 0.012} & 0.67 $\pm$ {\small 0.009} & 0.847 $\pm$ {\small 0.01} & \textbf{0.924 $\pm$ {\small 0.019}} & 0.72 $\pm$ {\small 0.036} \\
			
			LeastConf & 1 & 0.763 $\pm$ {\small 0.023} & 0.643 $\pm$ {\small 0.034} & 0.798 $\pm$ {\small 0.013} & 0.585 $\pm$ {\small 0.065} & 0.831 $\pm$ {\small 0.014} & 0.478 $\pm$ {\small 0.028} & 0.67 $\pm$ {\small 0.01} & \textbf{0.681 $\pm$ {\small 0.009}} & 0.819 $\pm$ {\small 0.023} & 0.921 $\pm$ {\small 0.019} & 0.675 $\pm$ {\small 0.072} \\
			
			DSA & 1 & 0.765 $\pm$ {\small 0.023} & 0.653 $\pm$ {\small 0.029} & 0.793 $\pm$ {\small 0.009} & 0.613 $\pm$ {\small 0.034} & 0.822 $\pm$ {\small 0.01} & 0.489 $\pm$ {\small 0.024} & 0.661 $\pm$ {\small 0.013} & 0.662 $\pm$ {\small 0.012} & 0.833 $\pm$ {\small 0.02} & \textbf{0.924 $\pm$ {\small 0.018}} & 0.718 $\pm$ {\small 0.033} \\
			
			BALD & 4 & \textbf{0.775 $\pm$ {\small 0.018}} & 0.641 $\pm$ {\small 0.034} & \textbf{0.801 $\pm$ {\small 0.013}} & 0.592 $\pm$ {\small 0.054} & 0.84 $\pm$ {\small 0.008} & 0.332 $\pm$ {\small 0.054} & \textbf{0.681 $\pm$ {\small 0.011}} & \textbf{0.681 $\pm$ {\small 0.013}} & 0.824 $\pm$ {\small 0.023} & 0.893 $\pm$ {\small 0.035} & 0.673 $\pm$ {\small 0.041} \\
			
			CoreGCN & 1 & 0.759 $\pm$ {\small 0.018} & 0.662 $\pm$ {\small 0.027} & 0.79 $\pm$ {\small 0.011} & 0.62 $\pm$ {\small 0.03} & 0.755 $\pm$ {\small 0.011} & 0.45 $\pm$ {\small 0.03} & 0.604 $\pm$ {\small 0.016} & 0.609 $\pm$ {\small 0.013} & 0.837 $\pm$ {\small 0.014} & 0.922 $\pm$ {\small 0.018} & \textbf{0.723 $\pm$ {\small 0.034}} \\
			
			Entropy & 1 & 0.765 $\pm$ {\small 0.022} & 0.66 $\pm$ {\small 0.03} & 0.798 $\pm$ {\small 0.011} & 0.611 $\pm$ {\small 0.054} & 0.823 $\pm$ {\small 0.013} & 0.464 $\pm$ {\small 0.024} & 0.663 $\pm$ {\small 0.013} & 0.672 $\pm$ {\small 0.011} & 0.801 $\pm$ {\small 0.025} & \textbf{0.924 $\pm$ {\small 0.02}} & 0.689 $\pm$ {\small 0.066} \\
			
			LSA & 0 & 0.769 $\pm$ {\small 0.016} & 0.654 $\pm$ {\small 0.032} & 0.781 $\pm$ {\small 0.013} & 0.61 $\pm$ {\small 0.041} & 0.82 $\pm$ {\small 0.009} & 0.484 $\pm$ {\small 0.022} & 0.617 $\pm$ {\small 0.012} & 0.641 $\pm$ {\small 0.011} & 0.816 $\pm$ {\small 0.012} & 0.915 $\pm$ {\small 0.018} & 0.718 $\pm$ {\small 0.038} \\
			
			Random & 0 & 0.758 $\pm$ {\small 0.015} & 0.655 $\pm$ {\small 0.026} & 0.771 $\pm$ {\small 0.013} & 0.623 $\pm$ {\small 0.031} & 0.82 $\pm$ {\small 0.009} & 0.476 $\pm$ {\small 0.024} & 0.616 $\pm$ {\small 0.016} & 0.637 $\pm$ {\small 0.012} & 0.812 $\pm$ {\small 0.014} & 0.921 $\pm$ {\small 0.018} & 0.713 $\pm$ {\small 0.034} \\
			
			Coreset & 1 & 0.765 $\pm$ {\small 0.017} & \textbf{0.663 $\pm$ {\small 0.023}} & 0.784 $\pm$ {\small 0.014} & 0.603 $\pm$ {\small 0.034} & 0.765 $\pm$ {\small 0.015} & 0.449 $\pm$ {\small 0.022} & 0.657 $\pm$ {\small 0.009} & 0.674 $\pm$ {\small 0.009} & 0.817 $\pm$ {\small 0.017} & 0.92 $\pm$ {\small 0.017} & 0.713 $\pm$ {\small 0.035} \\
			
			TypiClust & 0 & 0.759 $\pm$ {\small 0.014} & 0.641 $\pm$ {\small 0.028} & 0.775 $\pm$ {\small 0.01} & 0.603 $\pm$ {\small 0.04} & 0.757 $\pm$ {\small 0.02} & 0.465 $\pm$ {\small 0.027} & 0.596 $\pm$ {\small 0.014} & 0.567 $\pm$ {\small 0.012} & 0.727 $\pm$ {\small 0.026} & 0.916 $\pm$ {\small 0.02} & 0.693 $\pm$ {\small 0.045} \\
		\end{tabular}
	}
\end{table}
% QS 20
\begin{table}[H]
	\caption{AUC values for each dataset that supports query size 20.}
	\resizebox{\columnwidth}{!}{
		\begin{tabular}{l|llllllllll}
			& Wins & Splice       & SpliceEnc& DNA          & USPS         & USPSEnc  & Cifar10Enc& FMnistEnc & TopV2 & News \\
			\hline
			Oracle &  & 0.803 $\pm$ {\small 0.012} & 0.678 $\pm$ {\small 0.021} & 0.825 $\pm$ {\small 0.009} & 0.866 $\pm$ {\small 0.004} & 0.436 $\pm$ {\small 0.057} & 0.749 $\pm$ {\small 0.009} & 0.755 $\pm$ {\small 0.005} & 0.884 $\pm$ {\small 0.006} & 0.49 $\pm$ {\small 0.003} \\
			
			Margin & 1 & 0.759 $\pm$ {\small 0.027} & 0.618 $\pm$ {\small 0.04} & 0.779 $\pm$ {\small 0.013} & \textbf{0.847 $\pm$ {\small 0.008}} & 0.439 $\pm$ {\small 0.027} & 0.656 $\pm$ {\small 0.01} & 0.67 $\pm$ {\small 0.011} & 0.823 $\pm$ {\small 0.014} & 0.464 $\pm$ {\small 0.007} \\
			
			Badge & 3 & 0.767 $\pm$ {\small 0.013} & \textbf{0.619 $\pm$ {\small 0.033}} & 0.776 $\pm$ {\small 0.013} & 0.845 $\pm$ {\small 0.006} & \textbf{0.44 $\pm$ {\small 0.019}} & 0.647 $\pm$ {\small 0.013} & 0.665 $\pm$ {\small 0.007} & \textbf{0.827 $\pm$ {\small 0.016}} & 0.463 $\pm$ {\small 0.007} \\
			
			LeastConf & 1 & 0.751 $\pm$ {\small 0.02} & 0.597 $\pm$ {\small 0.05} & 0.748 $\pm$ {\small 0.025} & 0.798 $\pm$ {\small 0.027} & 0.391 $\pm$ {\small 0.024} & \textbf{0.665 $\pm$ {\small 0.013}} & 0.669 $\pm$ {\small 0.011} & 0.775 $\pm$ {\small 0.035} & 0.467 $\pm$ {\small 0.008} \\
			
			DSA & 0 & 0.759 $\pm$ {\small 0.02} & 0.599 $\pm$ {\small 0.034} & 0.769 $\pm$ {\small 0.013} & 0.809 $\pm$ {\small 0.012} & 0.421 $\pm$ {\small 0.023} & 0.647 $\pm$ {\small 0.014} & 0.63 $\pm$ {\small 0.013} & 0.793 $\pm$ {\small 0.026} & 0.459 $\pm$ {\small 0.01} \\
			
			BALD & 4 & \textbf{0.768 $\pm$ {\small 0.022}} & 0.57 $\pm$ {\small 0.037} & \textbf{0.784 $\pm$ {\small 0.015}} & 0.822 $\pm$ {\small 0.009} & 0.298 $\pm$ {\small 0.039} & 0.675 $\pm$ {\small 0.008} & \textbf{0.673 $\pm$ {\small 0.01}} & 0.789 $\pm$ {\small 0.024} & \textbf{0.468 $\pm$ {\small 0.009}} \\
			
			CoreGCN & 0 & 0.759 $\pm$ {\small 0.018} & 0.612 $\pm$ {\small 0.039} & 0.774 $\pm$ {\small 0.012} & 0.754 $\pm$ {\small 0.016} & 0.397 $\pm$ {\small 0.026} & 0.587 $\pm$ {\small 0.015} & 0.583 $\pm$ {\small 0.015} & 0.807 $\pm$ {\small 0.018} & 0.453 $\pm$ {\small 0.006} \\
			
			Entropy & 0 & 0.759 $\pm$ {\small 0.027} & 0.618 $\pm$ {\small 0.038} & 0.773 $\pm$ {\small 0.015} & 0.803 $\pm$ {\small 0.019} & 0.372 $\pm$ {\small 0.022} & 0.656 $\pm$ {\small 0.011} & 0.65 $\pm$ {\small 0.012} & 0.773 $\pm$ {\small 0.031} & 0.451 $\pm$ {\small 0.007} \\
			
			LSA & 0 & 0.761 $\pm$ {\small 0.014} & 0.611 $\pm$ {\small 0.039} & 0.768 $\pm$ {\small 0.015} & 0.816 $\pm$ {\small 0.009} & 0.411 $\pm$ {\small 0.022} & 0.621 $\pm$ {\small 0.01} & 0.635 $\pm$ {\small 0.011} & 0.796 $\pm$ {\small 0.016} & 0.452 $\pm$ {\small 0.007} \\
			
			Random & 0 & 0.755 $\pm$ {\small 0.014} & 0.612 $\pm$ {\small 0.039} & 0.763 $\pm$ {\small 0.012} & 0.818 $\pm$ {\small 0.009} & 0.439 $\pm$ {\small 0.019} & 0.622 $\pm$ {\small 0.013} & 0.633 $\pm$ {\small 0.012} & 0.795 $\pm$ {\small 0.016} & 0.45 $\pm$ {\small 0.006} \\
			
			Coreset & 0 & 0.759 $\pm$ {\small 0.016} & 0.601 $\pm$ {\small 0.034} & 0.764 $\pm$ {\small 0.015} & 0.757 $\pm$ {\small 0.015} & 0.39 $\pm$ {\small 0.029} & 0.647 $\pm$ {\small 0.009} & 0.651 $\pm$ {\small 0.011} & 0.784 $\pm$ {\small 0.026} & 0.435 $\pm$ {\small 0.012} \\
			
			TypiClust & 0 & 0.751 $\pm$ {\small 0.012} & 0.551 $\pm$ {\small 0.036} & 0.76 $\pm$ {\small 0.016} & 0.643 $\pm$ {\small 0.026} & 0.411 $\pm$ {\small 0.024} & 0.488 $\pm$ {\small 0.02} & 0.449 $\pm$ {\small 0.017} & 0.652 $\pm$ {\small 0.035} & 0.406 $\pm$ {\small 0.011} \\
		\end{tabular}
	}
\end{table}
% QS 50
\begin{table}[H]
	\caption{AUC values for each dataset that supports query size 50.}
	\resizebox{\columnwidth}{!}{
		\begin{tabular}{l|lllllllll}
			& Wins & Splice       & DNA          & USPS         & USPSEnc  & Cifar10Enc& FMnistEnc& TopV2     & News \\
			\hline
			Oracle &  & 0.803 $\pm$ {\small 0.012} & 0.825 $\pm$ {\small 0.009} & 0.866 $\pm$ {\small 0.004} & 0.436 $\pm$ {\small 0.057} & 0.749 $\pm$ {\small 0.009} & 0.755 $\pm$ {\small 0.005} & 0.884 $\pm$ {\small 0.006} & 0.49 $\pm$ {\small 0.003} \\
			
			Margin & 1 & 0.747 $\pm$ {\small 0.023} & 0.751 $\pm$ {\small 0.019} & 0.828 $\pm$ {\small 0.009} & 0.363 $\pm$ {\small 0.031} & 0.64 $\pm$ {\small 0.013} & \textbf{0.653 $\pm$ {\small 0.01}} & 0.774 $\pm$ {\small 0.029} & 0.46 $\pm$ {\small 0.006} \\
			
			Badge & 3 & 0.758 $\pm$ {\small 0.017} & 0.754 $\pm$ {\small 0.018} & \textbf{0.831 $\pm$ {\small 0.008}} & 0.376 $\pm$ {\small 0.028} & 0.632 $\pm$ {\small 0.013} & 0.649 $\pm$ {\small 0.011} & \textbf{0.781 $\pm$ {\small 0.026}} & \textbf{0.462 $\pm$ {\small 0.007}} \\
			
			LeastConf & 2 & 0.731 $\pm$ {\small 0.025} & 0.688 $\pm$ {\small 0.041} & 0.761 $\pm$ {\small 0.037} & 0.291 $\pm$ {\small 0.03} & \textbf{0.644 $\pm$ {\small 0.013}} & 0.65 $\pm$ {\small 0.011} & 0.73 $\pm$ {\small 0.049} & \textbf{0.462 $\pm$ {\small 0.009}} \\
			
			DSA & 0 & 0.748 $\pm$ {\small 0.021} & 0.738 $\pm$ {\small 0.018} & 0.783 $\pm$ {\small 0.016} & 0.346 $\pm$ {\small 0.027} & 0.624 $\pm$ {\small 0.014} & 0.588 $\pm$ {\small 0.016} & 0.748 $\pm$ {\small 0.041} & 0.45 $\pm$ {\small 0.011} \\
			
			BALD & 2 & \textbf{0.76 $\pm$ {\small 0.017}} & \textbf{0.756 $\pm$ {\small 0.018}} & 0.796 $\pm$ {\small 0.016} & 0.241 $\pm$ {\small 0.026} & 0.65 $\pm$ {\small 0.009} & 0.645 $\pm$ {\small 0.01} & 0.746 $\pm$ {\small 0.038} & 0.455 $\pm$ {\small 0.007} \\
			
			CoreGCN & 0 & 0.755 $\pm$ {\small 0.016} & 0.745 $\pm$ {\small 0.018} & 0.752 $\pm$ {\small 0.019} & 0.328 $\pm$ {\small 0.027} & 0.581 $\pm$ {\small 0.015} & 0.568 $\pm$ {\small 0.018} & 0.771 $\pm$ {\small 0.025} & 0.453 $\pm$ {\small 0.007} \\
			
			Entropy & 0 & 0.747 $\pm$ {\small 0.024} & 0.748 $\pm$ {\small 0.018} & 0.778 $\pm$ {\small 0.024} & 0.275 $\pm$ {\small 0.026} & 0.633 $\pm$ {\small 0.011} & 0.625 $\pm$ {\small 0.012} & 0.734 $\pm$ {\small 0.036} & 0.442 $\pm$ {\small 0.007} \\
			
			LSA & 0 & 0.754 $\pm$ {\small 0.013} & 0.749 $\pm$ {\small 0.019} & 0.807 $\pm$ {\small 0.01} & 0.341 $\pm$ {\small 0.029} & 0.613 $\pm$ {\small 0.012} & 0.625 $\pm$ {\small 0.01} & 0.763 $\pm$ {\small 0.025} & 0.45 $\pm$ {\small 0.006} \\
			
			Random & 1 & 0.746 $\pm$ {\small 0.012} & 0.745 $\pm$ {\small 0.015} & 0.806 $\pm$ {\small 0.008} & \textbf{0.379 $\pm$ {\small 0.028}} & 0.615 $\pm$ {\small 0.014} & 0.621 $\pm$ {\small 0.01} & 0.759 $\pm$ {\small 0.026} & 0.448 $\pm$ {\small 0.006} \\
			
			Coreset & 0 & 0.751 $\pm$ {\small 0.016} & 0.733 $\pm$ {\small 0.019} & 0.74 $\pm$ {\small 0.017} & 0.325 $\pm$ {\small 0.034} & 0.624 $\pm$ {\small 0.012} & 0.608 $\pm$ {\small 0.013} & 0.731 $\pm$ {\small 0.045} & 0.432 $\pm$ {\small 0.012} \\
			
			TypiClust & 0 & 0.749 $\pm$ {\small 0.016} & 0.736 $\pm$ {\small 0.016} & 0.586 $\pm$ {\small 0.038} & 0.348 $\pm$ {\small 0.027} & 0.451 $\pm$ {\small 0.024} & 0.375 $\pm$ {\small 0.022} & 0.614 $\pm$ {\small 0.046} & 0.397 $\pm$ {\small 0.012} \\
		\end{tabular}
	}
\end{table}
% QS 100
\begin{table}[H]
	\caption{AUC values for each dataset that supports query size 100.}
	\resizebox{\columnwidth}{!}{
		\begin{tabular}{l|llllllll}
			& Wins & Splice       & DNA          & USPS         & USPSEnc & Cifar10Enc& FMnistEnc& News \\
			\hline
			Oracle & 0 & 0.803 $\pm$ {\small 0.012} & 0.825 $\pm$ {\small 0.009} & 0.866 $\pm$ {\small 0.004} & 0.436 $\pm$ {\small 0.057} & 0.749 $\pm$ {\small 0.009} & 0.755 $\pm$ {\small 0.005} & 0.49 $\pm$ {\small 0.003} \\
			Margin & 2 & 0.733 $\pm$ {\small 0.024} & 0.711 $\pm$ {\small 0.027} & 0.799 $\pm$ {\small 0.013} & \textbf{0.473 $\pm$ {\small 0.026}} & 0.629 $\pm$ {\small 0.012} & \textbf{0.628 $\pm$ {\small 0.009}} & 0.455 $\pm$ {\small 0.006} \\
			Badge & 2 & 0.743 $\pm$ {\small 0.014} & 0.714 $\pm$ {\small 0.032} & \textbf{0.804 $\pm$ {\small 0.013}} & 0.472 $\pm$ {\small 0.029} & 0.623 $\pm$ {\small 0.01} & 0.621 $\pm$ {\small 0.01} & \textbf{0.456 $\pm$ {\small 0.006}} \\
			LeastConf & 0 & 0.715 $\pm$ {\small 0.033} & 0.639 $\pm$ {\small 0.05} & 0.708 $\pm$ {\small 0.034} & 0.23 $\pm$ {\small 0.034} & 0.631 $\pm$ {\small 0.013} & 0.62 $\pm$ {\small 0.012} & 0.457 $\pm$ {\small 0.008} \\
			DSA & 0 & 0.729 $\pm$ {\small 0.021} & 0.697 $\pm$ {\small 0.031} & 0.753 $\pm$ {\small 0.021} & 0.427 $\pm$ {\small 0.028} & 0.609 $\pm$ {\small 0.013} & 0.546 $\pm$ {\small 0.017} & 0.442 $\pm$ {\small 0.01} \\
			BALD & 3 & \textbf{0.744 $\pm$ {\small 0.015}} & \textbf{0.718 $\pm$ {\small 0.024}} & 0.765 $\pm$ {\small 0.021} & 0.285 $\pm$ {\small 0.046} & \textbf{0.632 $\pm$ {\small 0.009}} & 0.609 $\pm$ {\small 0.01} & 0.444 $\pm$ {\small 0.007} \\
			CoreGCN & 0 & 0.742 $\pm$ {\small 0.015} & 0.713 $\pm$ {\small 0.025} & 0.744 $\pm$ {\small 0.019} & 0.433 $\pm$ {\small 0.032} & 0.583 $\pm$ {\small 0.013} & 0.554 $\pm$ {\small 0.015} & 0.448 $\pm$ {\small 0.007} \\
			Entropy & 0 & 0.733 $\pm$ {\small 0.023} & 0.713 $\pm$ {\small 0.031} & 0.743 $\pm$ {\small 0.026} & 0.395 $\pm$ {\small 0.037} & 0.618 $\pm$ {\small 0.012} & 0.59 $\pm$ {\small 0.012} & 0.432 $\pm$ {\small 0.007} \\
			LSA & 0 & 0.738 $\pm$ {\small 0.017} & 0.716 $\pm$ {\small 0.027} & 0.789 $\pm$ {\small 0.011} & 0.439 $\pm$ {\small 0.03} & 0.609 $\pm$ {\small 0.013} & 0.608 $\pm$ {\small 0.01} & 0.447 $\pm$ {\small 0.006} \\
			Random & 0 & 0.733 $\pm$ {\small 0.013} & 0.713 $\pm$ {\small 0.023} & 0.789 $\pm$ {\small 0.012} & 0.468 $\pm$ {\small 0.024} & 0.611 $\pm$ {\small 0.01} & 0.606 $\pm$ {\small 0.01} & 0.446 $\pm$ {\small 0.005} \\
			Coreset & 0 & 0.735 $\pm$ {\small 0.019} & 0.698 $\pm$ {\small 0.026} & 0.721 $\pm$ {\small 0.021} & 0.396 $\pm$ {\small 0.024} & 0.608 $\pm$ {\small 0.012} & 0.562 $\pm$ {\small 0.016} & 0.426 $\pm$ {\small 0.012} \\
			TypiClust & 0 & 0.733 $\pm$ {\small 0.016} & 0.704 $\pm$ {\small 0.025} & 0.592 $\pm$ {\small 0.042} & 0.427 $\pm$ {\small 0.027} & 0.501 $\pm$ {\small 0.02} & 0.338 $\pm$ {\small 0.02} & 0.383 $\pm$ {\small 0.012} \\
		\end{tabular}
	}
\end{table}
\begin{minipage}{0.47\linewidth}
	% QS 500
	\begin{table}[H]
		\caption{AUC values for each dataset that supports query size 500.}
		\begin{tabular}{l|ll}
			& Cifar10 & FashionMnist \\
			\hline
			Oracle & 0.689 $\pm$ {\small 0.001} & 0.905 $\pm$ {\small 0.001} \\
			Margin & 0.556 $\pm$ {\small 0.008} & 0.882 $\pm$ {\small 0.004} \\
			Badge & 0.56 $\pm$ {\small 0.008} & 0.883 $\pm$ {\small 0.005} \\
			LeastConf & \textbf{0.591 $\pm$ {\small 0.01}} & \textbf{0.884 $\pm$ {\small 0.005}} \\
			DSA & 0.56 $\pm$ {\small 0.009} & 0.882 $\pm$ {\small 0.004} \\
			BALD & 0.478 $\pm$ {\small 0.014} & 0.878 $\pm$ {\small 0.003} \\
			CoreGCN & 0.553 $\pm$ {\small 0.01} & 0.88 $\pm$ {\small 0.007} \\
			Entropy & 0.553 $\pm$ {\small 0.009} & 0.882 $\pm$ {\small 0.006} \\
			LSA & 0.558 $\pm$ {\small 0.01} & 0.866 $\pm$ {\small 0.005} \\
			Random & 0.557 $\pm$ {\small 0.01} & 0.863 $\pm$ {\small 0.005} \\
			Coreset & 0.553 $\pm$ {\small 0.007} & 0.878 $\pm$ {\small 0.006} \\
			TypiClust & 0.557 $\pm$ {\small 0.009} & 0.864 $\pm$ {\small 0.004}
		\end{tabular}
	\end{table}
\end{minipage}
\hspace{10mm}
\begin{minipage}{0.47\linewidth}
	% QS 1000
	\begin{table}[H]
		\caption{AUC values for each dataset that supports query size 1000.}
		\begin{tabular}{l|ll}
			& Cifar10 & FashionMnist \\
			\hline
			Oracle & 0.689 $\pm$ {\small 0.001} & 0.905 $\pm$ {\small 0.001} \\
			Margin & 0.56 $\pm$ {\small 0.011} & 0.872 $\pm$ {\small 0.007} \\
			Badge & \textbf{0.562 $\pm$ {\small 0.013}} & 0.871 $\pm$ {\small 0.007} \\
			LeastConf & 0.561 $\pm$ {\small 0.012} & \textbf{0.873 $\pm$ {\small 0.006}} \\
			DSA & 0.56 $\pm$ {\small 0.011} & 0.87 $\pm$ {\small 0.008} \\
			BALD & 0.535 $\pm$ {\small 0.011} & 0.866 $\pm$ {\small 0.003} \\
			CoreGCN & 0.557 $\pm$ {\small 0.011} & 0.867 $\pm$ {\small 0.012} \\
			Entropy & 0.557 $\pm$ {\small 0.014} & 0.871 $\pm$ {\small 0.009} \\
			LSA & 0.551 $\pm$ {\small 0.012} & 0.854 $\pm$ {\small 0.009} \\
			Random & 0.55 $\pm$ {\small 0.01} & 0.855 $\pm$ {\small 0.006} \\
			Coreset & 0.562 $\pm$ {\small 0.012} & 0.869 $\pm$ {\small 0.004} \\
			TypiClust & 0.552 $\pm$ {\small 0.011} & 0.854 $\pm$ {\small 0.009} 	
		\end{tabular}
	\end{table}
\end{minipage}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Analysis of Results for the Semi-Supervised Domain}\label{app:semi_sup_analysis}
Even though the results for the aggregated semi-supervised domain appear in line with our overall ranking of methods, we observe stark differences for the sub-domains of semi-supervised image and semi-supervised tabular. \\
While semi-supervised images seem to mostly mirror the results from the normal image domain (with the exception of BALD), semi-supervised tabular data display highly irregular behavior, placing random sampling as second-best method behind the margin sampling.
Our oracle method even falls behind other methods.
Almost all methods a bunched into one region in the CD diagram with many non-significance bars indicating few, if any, significant differences between the methods. \\
Both the reasons, for the sub-random performance of most methods, and the bad performance of our oracle are currently unknown and require further research.
\begin{figure}[H]
	\centering
	\caption{Results for the semi-supervised domain, aggregated over all data types (top) and separately for images and tabular (bottom)}
	\includegraphics[width=0.6\linewidth]{img/macro_enc.jpg}
	\includegraphics[width=0.49\linewidth]{img/macro_enc_img.jpg}
	\includegraphics[width=0.49\linewidth]{img/macro_enc_tabular.jpg}
\end{figure}




\section{Hyperparameters per AL Method}\label{app:agent_hyperparameters}
\begin{table}[H]
    \caption{Selected hyperparameters for all tested AL methods. Last column indicates the source of our implementation.}
	\centering
	\begin{tabular}{l || l | l | l}
		Method & \makecell{Sample\\Size} & Other & Source\\
		\hline
		BADGE & 100 & & Based on \cite{ashdeep, curelab}\\
		BALD & 100 & Dropout Trials: 5 & Based on \cite{pycls} \\
		Coreset & 8000 & & Own \\
		TypiClust & 10000 & \makecell[tl]{Min Cluster Size: 5\\Max \# Clusters: 500} & Based on \cite{hacohen2022active} \\
		Margin & 8000 & & Own\\
		Entropy & 8000 &  & Own\\
	\end{tabular}
\end{table}




\section{Amount of Computational Resources Invested}\label{app:compute}
For our results we computed a total of 24200 runs (without Oracle runs) over a span of ~4 months. \\
We used our computational cluster consisting of 30-40 GPUs. \\ [1mm]
Number of runs per dataset and query size : 11 alg. * 50 runs = 550 \\ [1mm]
Runs per dataset: \\
 Cifar10   x2 = 1100\\
 Cfr10Enc  x4 = 2200\\
 DivSin    x2 = 1100\\
 DNA       x4 = 2200\\
 DNAEnc    x2 = 1100\\
 FMnist    x2 = 1100\\
 FMnistEnc x4 = 2200\\
 News      x3 = 1650\\
 Splice    x4 = 2200\\
 SpliceEnc x3 = 1650\\
 Honeypot  x2 = 1100\\
 TopV2     x4 = 2200\\
 USPS      x4 = 2200\\
 USPSEnc   x4 = 2200\\
 = 24200 runs
 
 
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \section{Limitations and Future Work}\label{app:future_work}
 Even though our benchmark includes a wide range of data domains, the number of datasets per domain is still limited.
 It remains untested if our selected datasets are indeed a good representation of their domain, or if additional datasets would skew the results of Fig. \ref{fig:ranks_by_domain}. \\ 
 Additionally, since we began working on this benchmark a few new AL Methods have been published.
 We consciously focused on only those methods for which good results have been reported by multiple sources, consequently omitting the newest methods. \\
 Most obviously, our future work involves the implementation of more datasets per domain and the newest AL methods. \\ [1mm]
 The choice of ranks for the main result table, like any other choice, has an impact in the interpretation of the results.
 E.g. comparing the rank of BALD in Table \ref{tab:results} (rank 6.6) with the amount of wins it is able to obtain in the AUC-based tables in Appendix \ref{app:auc_by_query_size}, suggests that an evaluation purely based on mean AUC values would count BALD to the best methods. \\
 We advocate for using ranks in AL evaluations for two reasons: (i) they are more robust to outlier performances in single runs and (ii) they highlight wether an method is able to consistently outperform another method, even if the difference in mean AUC is very small. \\
 Nonetheless, we think that the topic of truly fair evaluations for AL needs further research. \\ [1mm]
 Lastly, it remains untested if the differences between domains that we observe, are truly caused by the differences in data, but could also be influenced by the type of model that is common to those domains. \\
 An important part of our future work is therefore to test different model archetypes per domain.
 

\end{document}
